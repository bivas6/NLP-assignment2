{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Word Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "#### Q1.1 Evaluation Metrics ####\n",
    "\n",
    "## Input: y_pred, a list of length n with the predicted labels,\n",
    "## y_true, a list of length n with the true labels\n",
    "\n",
    "## Calculates the precision of the predicted labels\n",
    "\n",
    "def get_precision(y_pred, y_true):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 1 and y_true[i] == 1:\n",
    "            tp += 1\n",
    "        elif y_pred[i] == 1 and y_true[i] == 0:\n",
    "            fp +=1\n",
    "    return tp/(tp+fp)\n",
    "    \n",
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true):\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 1 and y_true[i] == 1:\n",
    "            tp += 1\n",
    "        elif y_pred[i] == 0 and y_true[i] == 1:\n",
    "            fn += 1\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true):\n",
    "    pre = get_precision(y_pred, y_true)\n",
    "    rec = get_recall(y_pred, y_true)\n",
    "    return 2*(pre*rec)/(pre+rec) # harmonic averasge of precision and recall measures\n",
    "\n",
    "def get_all_metrics(y_pred, y_true):\n",
    "    return [get_precision(y_pred, y_true), get_recall(y_pred, y_true), get_fscore(y_pred, y_true)]\n",
    "\n",
    "def test_predictions(y_pred, y_true):\n",
    "    print(\"precision is {}\".format(get_precision(y_pred, y_true)))\n",
    "    print(\"recall is {}\".format(get_recall(y_pred, y_true)))\n",
    "    print(\"fscore is {}\".format(get_fscore(y_pred, y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check 1.1\n",
    "pred_labels = [1,0,1,0,1]\n",
    "true_labels = [1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to get precision of 100%, recall of 60%, and fscore 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 1.0\n",
      "recall is 0.6\n",
      "fscore is 0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "test_predictions(pred_labels, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = [1,1,1,1,1]\n",
    "true_labels = [1,0,1,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to get precision of 60%, recall of 100%, and fscore 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.6\n",
      "recall is 1.0\n",
      "fscore is 0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "test_predictions(pred_labels, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to get 100% score on 1.1  :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads in the words and labels of one of the datasets\n",
    "def load_file(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1: Majority Class Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2.1: A very simple baseline\n",
    "\n",
    "## Labels every word complex\n",
    "def all_complex(data_file):\n",
    "    words, labels = load_file(data_file)\n",
    "    return get_all_metrics([1]*len(labels), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we should get perfect recall. Precision and fscore which is affected by the precision, are dependent on the number of complex (label '1') words in the dataset. In fact, the precision will tell us the percentage of 'complex' words in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.43275, 1.0, 0.604083057058105]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_complex(\"../data/complex_words_training.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.418, 1.0, 0.5895627644569816]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_complex(\"../data/complex_words_development.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, perfect accuracy. Training dataset has 43% complex words, and development set only 41%. As it happens, our classifier isn't actually 'magority classfier'. If we would choose to predict '0' for all the words, we will improve on both training and development sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2: Word Length Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to find the word length threshold. We expect to find such a threshold, due to our prior knowledge of the language - we know that, in general, longer words are less known and more 'complex'. This is probably because the way a langauge is evolved: at first, people took short words to signal meanings which were commonly used, as it is makes communication shorter and simpler. Derivations, inflections and other greens came later on. Therefore, we expect that at some word length and above, (almost) every word will be classified as 'complex'. This is the proof that there is a threshold and it isn't a monotonic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_length_classifier(file, length):\n",
    "    words, labels = load_file(file)\n",
    "    y_pred = [1 if len(w) >= length else 0 for w in words]               \n",
    "    return get_all_metrics(y_pred, labels)\n",
    "\n",
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "## Also, draws a graph for recall X precision for the training data\n",
    "def word_length_threshold(training_file, development_file):\n",
    "    threshold = -1\n",
    "    best_fscore = 0.0\n",
    "    best_training_peformance = []\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for word_length in range(20):  # Assuming by prior knowledge of English, \n",
    "                                    # that the threshold must be in that range\n",
    "        training_performance = word_length_classifier(training_file, word_length)\n",
    "        if best_fscore < training_performance[2]:\n",
    "            best_fscore = training_performance[2]\n",
    "            threshold = word_length\n",
    "            best_training_peformance = training_performance\n",
    "        xs.append(training_performance[1])\n",
    "        ys.append(training_performance[0])\n",
    "    \n",
    "    plt.plot(xs, ys)\n",
    "    plt.xlabel('recall')\n",
    "    plt.ylabel('precision')\n",
    "    plt.title('percision and recall as the word length reduces')\n",
    "    plt.show()\n",
    "    round_xs = [round(x,2) for x in xs]\n",
    "    round_ys = [round(y,2) for y in ys]\n",
    "    print (\"recall:\\n{}\\nprecision: \\n{}\".format(round_xs, round_ys))\n",
    "    print (\"Threshold is {}\".format(threshold))\n",
    "    \n",
    "    development_performance = word_length_classifier(development_file, threshold)\n",
    "\n",
    "    return best_training_peformance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we consider all relevant word length range, we should expect the following: clearly, for length of 0, the classifier labels all words with '1', and consequently equals the 'majority classifier'. Actually, since we expect words of length 1 and 2 to be all simple, the appropriate calssifiers are also 'majority classifiers'. We would expect them to have perfect recall and 43% precision, according the results we got above. On the other side of the scale, we would expect to get very poor recall for word length of 15-19, since these lengths implies all labels to be '0' and no true prediction is made. The recall should be monotonically decreasing with the increase of word length, due to the fact that we're 'hitting' less '1's or 'complex' words out of the total '1's. Precision should have a peak where the tradeoff between being cautious [=predicting '1's only when we're sure, i.e. for long words] and being reckless [=predicting too many '1's, even for very short words] is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:\n",
      "[1.0, 1.0, 1.0, 1.0, 0.99, 0.97, 0.93, 0.84, 0.71, 0.54, 0.36, 0.22, 0.13, 0.07, 0.04, 0.02, 0.01, 0.01, 0.0, 0.0]\n",
      "precision: \n",
      "[0.43, 0.43, 0.43, 0.43, 0.44, 0.48, 0.54, 0.6, 0.68, 0.75, 0.77, 0.78, 0.79, 0.78, 0.75, 0.75, 0.69, 0.76, 0.7, 0.5]\n",
      "Threshold is 7\n",
      "For the threshold [precision, recall, f_score]:\n",
      " train - 0.601 0.844 0.702\n",
      " dev - 0.605 0.866 0.713\n"
     ]
    }
   ],
   "source": [
    "best_train_perf, dev_perf = word_length_threshold(\"../data/complex_words_training.txt\", \"../data/complex_words_development.txt\")\n",
    "print (\"For the threshold [precision, recall, f_score]:\\n train - {0:.3f} {1:.3f} {2:.3f}\\n dev - {3:.3f} {4:.3f} {5:.3f}\"\\\n",
    "       .format(best_train_perf[0], best_train_perf[1], best_train_perf[2], dev_perf[0], dev_perf[1], dev_perf[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, our predictions were correct. The minor decrease in the left part of the graph, is probably caused by a defection in the data: one of the longer words groups (lenght of 16) had relatively few complex words in it, and the precision declined momentarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3: Word frequency thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads Google NGram counts\n",
    "def load_ngram_counts(ngram_counts_file): \n",
    "   counts = defaultdict(int) \n",
    "   with gzip.open(ngram_counts_file, 'rt', encoding=\"utf8\") as f: \n",
    "       for line in f:\n",
    "           token, count = line.strip().split('\\t') \n",
    "           if token[0].islower(): \n",
    "               counts[token] = int(count) \n",
    "   return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we classify the complexity of words according their used frequency. The 'crazy know all' Google collected such information, and we use a sample of more than 8 million words with their appropriate frequency (probably, the number of times each appeared in a huge text corpus). We assume that if a word is frequent, it implies that many people use it, and therefore know it and regard them as 'readable' for other people as well. In short, many people treat this words as 'simple' to some extent. In fact, in some way, it extends hugely our data and development sets (although, without actual labels).\n",
    "We expect to see similar results as in the word length threshold: recall will be perfect above a specific chosen frequency, since we will label everything as complex (or at least, everthing which is only a little complex and above, which, in practice, will be all the words we are looking at). Since 'The' appears around 50 billion times (in all it's forms: 'the', 'The', 'THE') in the Google corpus, it seems rational to think that the best threshold will be under 250 million (because of the huge decrease in the frequency function). We'll look for it in jumps of 10 millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:\n",
      "[0.04, 0.69, 0.82, 0.87, 0.92, 0.94, 0.96, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "precision: \n",
      "[0.38, 0.6, 0.56, 0.53, 0.52, 0.51, 0.5, 0.49, 0.48, 0.48, 0.47, 0.47, 0.46, 0.46, 0.46, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.44, 0.44, 0.44]\n",
      "Threshold is 20000000\n",
      "For the threshold [precision, recall, f_score]:\n",
      " train - 0.564 0.816 0.667\n",
      " dev - 0.556 0.844 0.670\n"
     ]
    }
   ],
   "source": [
    "def word_frequency_classifier(file, freq):\n",
    "    global counts\n",
    "    words, labels = load_file(file)\n",
    "    y_pred = [1 if counts[w] <= freq else 0 for w in words]\n",
    "    return get_all_metrics(y_pred, labels)\n",
    "\n",
    "# Finds the best frequency threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_frequency_threshold(training_file, development_file, counts):\n",
    "    threshold = -1\n",
    "    best_fscore = 0.0\n",
    "    best_training_peformance = []\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for word_freq in range(0, 250000000, 10000000):\n",
    "        training_performance = word_frequency_classifier(training_file, word_freq)\n",
    "        if best_fscore < training_performance[2]:\n",
    "            best_fscore = training_performance[2]\n",
    "            threshold = word_freq\n",
    "            best_training_peformance = training_performance\n",
    "        xs.append(training_performance[1])\n",
    "        ys.append(training_performance[0])\n",
    "    \n",
    "    plt.plot(xs, ys)\n",
    "    plt.xlabel('recall')\n",
    "    plt.ylabel('precision')\n",
    "    plt.title('percision and recall as the word frequency grows')\n",
    "    plt.show()\n",
    "    round_xs = [round(x,2) for x in xs]\n",
    "    round_ys = [round(y,2) for y in ys]\n",
    "    print (\"recall:\\n{}\\nprecision: \\n{}\".format(round_xs, round_ys))\n",
    "    print (\"Threshold is {}\".format(threshold))\n",
    "    \n",
    "    development_performance = word_frequency_classifier(development_file, threshold)\n",
    "\n",
    "    return best_training_peformance, development_performance\n",
    "\n",
    "counts = load_ngram_counts('../data/ngram_counts.txt.gz')\n",
    "best_train_perf, dev_perf = word_frequency_threshold(\"../data/complex_words_training.txt\",\\\n",
    "                                                     \"../data/complex_words_development.txt\", counts)\n",
    "print (\"For the threshold [precision, recall, f_score]:\\n train - {0:.3f} {1:.3f} {2:.3f}\\n dev - {3:.3f} {4:.3f} {5:.3f}\"\\\n",
    "       .format(best_train_perf[0], best_train_perf[1], best_train_perf[2], dev_perf[0], dev_perf[1], dev_perf[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the function has the predicted shape, the fine straight line on half the graph seems a little shady. Closer look reveals, that there's a huge jump in recall between word_freq of 0 to 10000000, which arouses suspicion that our search gap was too large. We'll to refine our search.\n",
    "Note, that for word freq of 0, we didn't get 0 recall, although we label all the words as 'simple', unless their frequency is 0. That would mean Google didn't see them. It seemed a little weird at first, but after the following check, we had to accept that some of the 'complex' words weren't seen by Google, and therefore are probably not real words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words from the training set that Google didn't see is 164\n"
     ]
    }
   ],
   "source": [
    "words, _ = load_file(\"../data/complex_words_training.txt\")\n",
    "counter = 0\n",
    "for w in words:\n",
    "    if counts[w] == 0:\n",
    "        counter+=1\n",
    "print (\"Number of words from the training set that Google didn't see is {}\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xl8FPX5wPHPk5uEhDPcgXDKfQYQ8D5RLFRBBS+01quKWq2t9rDW41erba33hQd4oQ1qqaLUC+WGhBsEDYGQQIAQbgKBJM/vj5ngEhJ2A9ndZPd5v177yu7Md2afmZ3dJ/OdeWZEVTHGGGOOJyLYARhjjKn9LFkYY4zxypKFMcYYryxZGGOM8cqShTHGGK8sWRhjjPHKkkWQicjVIvI/H9q9JCJ/CkRMJ0pEUkVERSQqyHE8JCJvBzMmEbleRGYH8j1Plrd1JSKniMgSEdkrIncGOj4TXEH9UhtQ1XeAd3xod2sAwjEnQERSgfVAtKqWBDcav/otMFNV+wU7EBN4tmfhZ8H+L7u2svUSXCe4/tsBq44zz8gTjyi0hOL2bcmiCiKyQUQeEJHVIrJTRN4QkTiP8ZeIyFIR2SUic0Wkd4Vpfyciy4H9IhIlIiki8qGIFIhIoYg857Y90l0hjqdEZJuI7BaR5SLS0x33pog86vEeN4lIlojsEJFpItLKY5yKyK0i8qMb+/MiIlUs5yARmecuR76IPCciMb7MS0QiReTvIrJdRLKBET6s04rrpZWITHXXy3rP7g13/r8XkXVu10emiKS4454WkVwR2eMOP92nD/bYmO73mP9qEbnUY1wnEfnW/Sy2i8j7VczmO/fvLhHZJyJDPObxd3e9rReRizyGNxCR19x1vklEHq3sx1ZE4kTkgIg0dV//UURKRCTJff2oiPzLY56T3XWZ47aNcMddLyJz3O1rB/BQdT4/EfkaOBt4zl3GLu42+aKITBeR/cDZIhLrznOjiGwVp/u0nsd87nOXebOI/MLdvjq542aKyC892h7VlSciXUXkC3ebXysiV3iMe9PdNj91P8sFItLRY3wPj2m3uttVCxEpEpEmHu0GuOsvupJ1UE9EJrmf5/ci8lsRyfMYX9n23c1drl0iskpERrpt27vDyj+fiSKyzWNeb4vI3R7rIdtdrvUicnVVn5Nfqao9KnkAG4CVQArQGJgDPOqO6w9sAwYDkcB4t32sx7RL3WnruW2WAU8BCUAccJrb9npgtvv8QiATaAgI0A1o6Y570+P9zwG2u3HEAs8C33nErsAn7nzaAgXA8CqWcwBwKk6XZCrwPXC3L/MCbgXWeKyjb9z2UcdZp57rJcJd3geBGKADkA1c6La/D1gBnOKujz5AE3fcNUATN+57gS1AnDvuIeBt93mql5guB1q5sVwJ7PdY5+8Bf3DHHfnMKpnHMe/hfq6HgZvcz/82YDMg7viPgZfd7aEZsBC4pYr5fweMdp//D1gHXOQx7lL3+WTgP0CiG9MPwI0e8ZQAE9x1Vu8EPr+ZwC89Xr8J7AaGeayjfwHT3PklAv8F/uq2Hw5sBXq6y/2u+36dqpj/9fz03UgAcoEb3Pj743wHenjEsgMY5I5/B5jijksE8nG2kzj39WB33HTgNo/3fAp4torlfxz4FmgEtAGWA3nH2b6jgSzg9zjb9znAXuAUt/1GYID7fC3Ott/NY1w/d7n3eEzTsnyZA/6bGIw3rQsP94O/1eP1xcA69/mLwCMV2q8FzvSY9hce44bg/Mge8yWs8IU4B+cLfioQUaHdm/yULF4DnvAYVx/nhynVfa14/LABHwD3+7jcdwMfebyucl7A1xXW0QV4Txae62UwsLFCmweANzzW6Sgf494J9HGfP4SPyaKS+Swtf0+cH99XgDZepjnmPdzPNcvjdbzbpgXQHCgG6nmMHwd8U8X8HwGewfkR3ALchfPDFQccAJriJKRioLvHdLfgHGMoj6fiuq7u5zeTY5PFZI/XgpNsO1bY9te7z18HHvcY1wXfk8WVwKwK8bwM/NkjlokVvq9rPNbtkiqW6Upgjvs80l2/g6poe+QfGff1Lzk2WXhu36e784vwGPYe8JD7/C3gHnebWAs8gZPA2wO7cBJwgvt8tOf2EoyHdUMdX67H8xyc/0DB6bu9192N3CUiu3D+m2hVxbQpQI56Ofipql8DzwHPA1tF5JXy7oYKWrnxlE+3DygEWnu02eLxvAgnoRzD7U74RES2iMge4P9wfnw8VTWvVhy7jrzxbN8OaFVhPf4e58cUnPW2roq473W7Ana70zWoJG6vROQ6+ak7cRfOf73l8/ktzg/gQrcL4RfVnP2R9aaqRe7T+jjLHQ3ke7zvyzh7GJX5FjgL57/pFcAXwJk4/1Rkqep2N+YYjv4Mcjh6m/Bc93Bin19FntMn4yTFTI/l+twdfrLv1w4YXGFbuRrnh7ZcVdtpldsRzp5YdxHpAJwP7FbVhVW0rRh/xfVZcVgrIFdVyzyGeX4m5Z/rGTh7iDNxPtczcRJjmarux0lot+JsL5+KSNcq4vMrSxbHl+LxvC1ONwI4G8RjqtrQ4xGvqu95tPe8nG8u0FZ8OOilqs+o6gCgB85/XvdV0mwzzpcHABFJwOmS2eTLQlXwIk5XRGdVTcL5sa70+EYl8jl2HXlTcb2sr7AeE1X1Yo/xHSvOQJzjE78DrgAaqWpDnO4QX+Mun0874FXgDpzurYY4XY8CoKpbVPUmVW2F81/6C+X968dZJl/k4uwFNPVY7iRV7VFF+7k4XXGXAt+q6mqcdT0C5wcHnC6Zw3hsF24bz22iYpwn8vlV5DnP7Th7Oj08lquBqpb/aHt7v/04yaacZyLIxVl2z22lvqre5kOMlW5HAKp6EGdv+WrgWpz/9quSj9P9VC6lkjae62MzkFJ+XMLl+Zl8i7P3cZb7fDZOl96Z/PS5oqozVPV8nC6oNTjbbMBZsji+20WkjYg0xvkRLT/A+Spwq4gMFkeCiIwQkcQq5rMQZ0N73G0bJyLDKjYSkYHuPKNxvjgHgdJK5vcucIOI9BWRWJy9gQWquuEEljERp090n/sfiy9fvnIfAHe666gRcH8133shsMc9KFhPnAOuPUVkoDt+IvCIiHR213Nv92BkIk7/ewEQJSIPApXtgXmTgPPlLgAQkRtw9ixwX18uIuU/DjvdtpV9HgVAGc4xF69UNR/n2MM/RCRJRCJEpKOInFlF+yKcYzu389OPyFycBPat26YU5/N4TEQS3UR4D/D2cUI52c+vYpxlON+Np0SkGYCItBaRCz3e73oR6S4i8cCfK8xiKXCZiMS7SflGj3GfAF1E5FoRiXYfA0Wkmw+hfQK0EJG7xTkAnygigz3GT8bp8hqJ9/X1gIg0EpHWOP9kHM8CnO/xb914zwJ+BkwBUNUfcZLrNTjHHPfgHNMZjfu5ikhzERnp/kNYDOyj8m3Q7yxZHN+7OF/qbPfxKICqZuAcuHwO50ckC2djq5T7Rf4Z0AnnwFUezq5lRUk4X7adOLurhcDfK5nfV8CfgKk4SagjMLb6iwfAb4CrcA68vcpPCdEXrwIzcA7eLwY+rM4be6yXvjh1CttxEkQDt8k/cb6g/8NJaK/hHDicAXyGc3wnByepVtYl4O39VwP/AObhfEl74ZzIUG4gsEBE9uEctL1LVddXMp8i4DFgjttFcqoPb38dTrfRapzPOx3nP8eqfIvTdbXQ43UiP52JBc7B6/042+psnO339ePM86Q+vyr8Duf7MN/t1vwSZ68IVf0M5wD4126brytM+xRwCOezmIRH/ZGq7sU5pjIW5z/2LcDfcE7wOC532vNxtrUtwI84Z3aVj5+Dk+wXe/mH62Gc7+56d7nScX7Aq3rfQzgJ6CKcbfsF4DpVXePR7FugUFU3erwWYIn7OgLnwPxmnAP4ZwK/8rbM/lB+ZoapQEQ24Bxs+zLYsRgTqkREcbpAs4Icx9fAu6o6sRrT3AaMVdVK9whDje1ZGGPCmtvt2R8ve9Ui0lJEhrndhqfg/Mf/USBirA1CrsrQGGN8JSKTgJ/jdDHu9dI8BuestfJTW6fgdC2FBeuGMsYY45V1QxljjPEqZLqhmjZtqqmpqcEOwxhj6pTMzMztqprsrV3IJIvU1FQyMjKCHYYxxtQpIuJTJb11QxljjPHKkoUxxhivLFkYY4zxypKFMcYYryxZGGOM8cqShTHGGK8sWRhjjPEqZOosjKkOVWX/oVIK9hZ7PA6iwNiBbakXExnsEI2pVSxZmJBSXFLK9n2HKNhbzPa9xRTsKz46IXi8PnC48nvIfLZyC69fP5D6sfb1MKacfRtMnbD7wGE27zpQ6Y++5+vdBw5XOn3D+GiS68eSnBhLv7YNjzxv6v4tf8zJ2s49HyzjmokLmHTDIBrERwd4SY2pnSxZmFpLVVmwfgeT5m7gf6u3Ulp29BWS60VH0iwpluT6sXRuVp+hHZv89OPvkQSa1I8hNsq3bqVRfVsTFx3JhHeXMO7V+bx14yCa1Pd6MzZjQl7IXKI8LS1N7dpQoeHAoVI+XrqJSXM3sGbLXhrGR3NFWgp9UxoelQgS/NhN9O0PBdw8OYO2jeN555eDaZYU57f3MiaYRCRTVdO8trNkYWqL3B1FTJ63gfcX5bLnYAndWiZx/dB2jOzTOigHnOetK+TGSYtolhjLOzedSuuG9QIegzH+ZsnC1Amqyuys7Uyau4Gv1mwjQoThPVowfmgqA1MbISJBjW/xxp2Mf30hSXHRvPPLwaQ2TQhqPMbUtFqRLERkOPA0EAlMVNXHK2lzBfAQoMAyVb3KHV4KrHCbbVTVkcd7L0sWdcu+4hI+XJzHpLkbWFewnyYJMVw1uC1XD25Hiwa1q8tn5abdXPvaAqIjI3j3psF0apYY7JCMqTFBTxYiEgn8AJwP5AGLgHGqutqjTWfgA+AcVd0pIs1UdZs7bp+q1vf1/SxZ1A3ZBfuYPC+H9Mw89hWX0KdNA8YPTWVE75Y+H4QOhh+27uXqiQsoK1PeunEw3VslBTskY2qEr8nCn2dDDQKyVDXbDWgKMApY7dHmJuB5Vd0JUJ4oTGgpK1Nm/rCNN+fm8N0PBURHCiN6tWT80FT6tW0U7PB80qV5Ih/cMoSrX53P2FfmMfnGwfRNaRjssIwJGH8mi9ZArsfrPGBwhTZdAERkDk5X1UOq+rk7Lk5EMoAS4HFV/bjiG4jIzcDNAG3btq3Z6M1J233gMP/OyOWt+TnkFBbRLDGWe87vwthBKTRLrF1dTb5o3zSB928ZwtUTF3DNxAW8fv1ABrVvHOywjAkIfyaLyo5MVuzzigI6A2cBbYBZItJTVXcBbVV1s4h0AL4WkRWquu6omam+ArwCTjdUTS+AOTE/bN3LpLkb+HDxJg4cLiWtXSN+c8EpDO/ZgujIun05spTG8c4exsT5XPf6Al69Lo3TO3u9fbExdZ4/k0UekOLxug2wuZI281X1MLBeRNbiJI9FqroZQFWzRWQm0A9Yh6mVSkrL+PL7bUyau4F52YXEREUwqk8rxg9NpWfrBsEOr0a1aBDH+7cM4ZqJC7jxzQxeuLo/53VvHuywjPErfx7gjsI5wH0usAnnAPdVqrrKo81wnIPe40WkKbAE6AuUAUWqWuwOnweM8jw4XpEd4A6OnfsPMWVRLm/Pz2HTrgO0bliPa05tx5UDU2icEBPs8PxqV9Ehxr++kFWb9/CvsX25pHerYIdkTLUF/QC3qpaIyB3ADJzjEa+r6ioReRjIUNVp7rgLRGQ1UArcp6qFIjIUeFlEynAuo/748RKFCbyVm3Yzed4G/rN0M8UlZQzp0IQ/XdKd87o1I6qOdzX5qmF8DG//cjC/eHMRd763hIOHyxgzoE2wwzLGL6woz/jscGkZn6/cwqS5G8jI2Um96Egu7d+a8UNSOaVF+NYeFB0q4ebJmczO2s6Dl3TnhmGpQS8mNMZXQd+zMKGjYG8x7y3cyDsLcti6p5i2jeP544huXD4gxa7KCsTHRDFxfBoT3lvCw5+sZtGGHTw+ujcN6tm6MaHDkoWp0pKNO5k8L4dPl+dzqLSMM7ok89fL2nFWl2ZERNh/zp7ioiN5+ZoBvDormydnrGV53iyevaof/etIHYkx3lg3lDlKcUkpny7PZ9LcDSzL20392CjGDGjDtUPa0THZ54L6sLZ4407ufG8JW3Yf5DcXnsLNp3ew5GpqraBf7iPQLFmcvJLSMkY8M5u1W/fSITmB8UNSGT2gjd0x7gTsPnCYBz5czvQVWzijSzL/vKIPTe2+GKYW8jVZhMdpK8Yns7K2s3brXh4e1YOv7jmT8UNTLVGcoAb1onn+qv48dmlPFmQXctHTs5iTtT3YYRlzwixZmCPSM/NoFB/N2IFt7WyeGiAiXD24Hf+5YxhJcVFc89oC/j5jLSWlZcEOzZhqs2RhANhddJgvVm1lVN/WxETZZlGTurZI4r8TTuPyAW147pssxr06n827DgQ7LGOqxX4VDADTlm/mUKkVlflLfEwUT4zpw9Nj+7J68x4uenoWX6zeGuywjPGZJQsDOF1QXVsk0sPu0+BXo/q25pM7TyelcT1umpzBQ9NWUVxSGuywjPHKkoXhx617WZa7izED2tixigBo3zSBqbcN5YZhqbw5dwPD/zWLZ776kXUF+4IdmjFVslNdDOmL84iMEEb1bR3sUMJGbFQkf/5ZD07v3JSXZmbz1Jc/8M8vfqBbyyQu6d2Si3u1pL3d79vUIpYswlxJaRkfLd7E2ackk5xodQCBdk7X5pzTtTlbdh/ks5X5fLo8nydnrOXJGWvp0SqJEb1bMqJXS9o1scRhgsuSRZiblbWdbXuL7cB2kLVoEMcNw9pzw7D2bN51gOkr8vl0RT5PfL6WJz5fS6/WDY4kjpTG8cEO14Qhq+AOc3e8u5g5WdtZ8Pvz7JTZWihvZxGfrdjCJyvyWZa7C4DuLZMY2rEJQzs1YWBqYxLj7IKF5sTZVWeNV7uLDvO/1Vu5alBbSxS1VJtG8dx0RgduOqMDuTuKmL4in5lrC5g8P4eJs9cTGSH0at2AIR2bMLRjE9LaNaZeTGSwwzYhyJJFGPvv8s0cKrHairoipXE8t5zZkVvO7MjBw6Us3riTeesKmbeukFe/y+bFmeuIjhT6pTTiVDd59GvbkNgoSx7m5FmyCGNWW1F3xUVHMrRjU4Z2bArA/uISMnJ2MnfdduatK+S5r3/kma9+JDYqgkHtGzO8ZwuG92hBE7uYoTlBlizCVNa2vSzN3cUfR3Sz2ooQkBAbxZldkjmzSzLgXPV24fodzF23nZlrC/jDRyv508crGdKxCSN6teLCHs0tcZhqsQPcYerxz9bw6qxs5j9wrp0yG+JUle/z9x45w2r99v1ERghDOjRhRO+WXNijBY0TYoIdpgkSu5+FqVJpmTL08a/o1boBE8cPDHY4JoBUldX5e5zEsTyfDYVFREYI53drzoRzO9GjVYNgh2gCzM6GMlWa9WMBW/cU89DP7MB2uBERerRqQI9WDfjNBaewOn8P05Zt5t0FG/l81RbO796cu87tTM/WljTM0SxZhKH0zDwaxkdzTrdmwQ7FBJFn4vjVWZ14Y856Xp+9nktWb+Xcrs2467zO9G7TMNhhmlrCTq4PM+W1FaP6tLJTKs0RDepFc/d5XZh9/znce34XMnJ2MvK5OdzwxkIWb9wZ7PBMLWB7FmHmp9qKlGCHYmqhpLhoJpzbmeuHpTJ5Xg6vzsrmshfm0rN1EmMHtmVk31YkWcV4WLID3GHm58/P4cChUj6/+3Q7ZdZ4ta+4hPSMXKYsymXNlr3ERUcwolcrxg5KIa1dI9uGQoAd4DbHyNq2j6W5u/jDxVZbYXxTPzaK64e1Z/zQVJbn7WbKolymLd3E1MV5dG+ZxCvXDaBNI7uwYTjw6zELERkuImtFJEtE7q+izRUislpEVonIux7Dx4vIj+5jvD/jDBdTy+9b0a9VsEMxdYyI0CelIX+9rBcL/3Aefxvdi7ydRYx+cS4/bN0b7PBMAPgtWYhIJPA8cBHQHRgnIt0rtOkMPAAMU9UewN3u8MbAn4HBwCDgzyLSyF+xhoPSMuXDxXmc1SWZZolxwQ7H1GEJsVFcObAt798yBFW4/KV5ZObsCHZYxs/8uWcxCMhS1WxVPQRMAUZVaHMT8Lyq7gRQ1W3u8AuBL1R1hzvuC2C4H2MNebOztrN1j923wtScbi2TmHrbUBrFR3P1xAV8s3ab94lMneXPZNEayPV4necO89QF6CIic0RkvogMr8a0iMjNIpIhIhkFBQU1GHrosdoK4w8pjeNJv20onZrV56ZJGby/aGOwQzJ+4s9kUdkR1IqnXkUBnYGzgHHARBFp6OO0qOorqpqmqmnJycknGW7o2n3gMDNWbbHaCuMXTevH8t5Np3Jqhyb8buoK7p+6nIOHS4Mdlqlh/kwWeYDnyfxtgM2VtPmPqh5W1fXAWpzk4cu0xkefWG2F8bPEuGgm/WIQt5/dkSmLchnz0lxydxQFOyxTg/yZLBYBnUWkvYjEAGOBaRXafAycDSAiTXG6pbKBGcAFItLIPbB9gTvMnID0zDxOaZ5Iz9Z23wrjP5ERwn0XdmXidWnkFBYx4plZfL1ma7DDMjXEb8lCVUuAO3B+5L8HPlDVVSLysIiMdJvNAApFZDXwDXCfqhaq6g7gEZyEswh42B1mqilr2z6WbNzF6AGtrbbCBMR53Zvz6YTTadMonl+8mcHt7y5mY6HtZdR1VsEd4v72+Rpe+S6befefQ7MkO2XWBM7Bw6W8OHMdr3yXTUlZGeOHpDLhnM40iLfLhdQmvlZw24UEQ1h5bcWZXZItUZiAi4uO5Nfnd2HmfWdxab/WvDZnPWc8+Q2vzV7PoZKyYIdnqsmSRQiz2gpTGzRPiuOJMX2Yfufp9G7TgEc+Wc3Pn59jXVN1jCWLEDY1M48G9aI512orTC3QrWUSk38xiJevHUDeziIueXYWX662A+B1hSWLEHWktqKv1VaY2kNEuLBHCz6983TaNonnl5MzeHLGGkrLQuPYaSizZBGiPl2eT3FJmXVBmVoppXE86bcOZdygFJ7/Zh3XvrbALkhYy1myCFHpmbl0aV6fXnYvZVNLxUVH8tfLevPEmN4sy93FBU99x61vZbJy0+5gh2YqYfezCEHrCvaxeOMufn9xV6utMLXeFWkpnN+tOW/MWc8bczfw+aotnH1KMned14W+KXYP8NrC9ixC0NRM574VP+97zLUXjamVGiXEcM8FpzDn/nO478JTWJq7i8temMNrs9cTKrVgdZ0lixDj1FZsstoKUyclxUVz+9mdmPW7czi/e3Me+WQ1909dYXUZtYAlixAzJ2s7W/YctAPbpk6rHxvFi1cP4M5zOvF+Ri7XTFxA4b7iYIcV1ixZhJh0q60wISIiQrjnglN4Zlw/luXtYuRzc1izZU+wwwpblixCSHltxUi7b4UJISP7tOKDW4ZwuLSM0S/M5Qsr5AsKSxYhxGorTKjqk9KQaXecRsdm9bn5rQxemJllB74DzJJFCEnPzKVzs/r0bmO1FSb0tGgQxwe3DOGS3q144vO13PvBMg6X2oHvQLFkESKy3dqKMQPaWG2FCVlx0ZE8M7Yv957fhQ+XbOL2dxbbmVIBYskiRExdnEeEwKX9rLbChDYRYcK5nfnLyB78b/VWbn070+75HQCWLEKA1VaYcDR+aCqPXdqTr9ds46bJGewvLgl2SCHNkkUImLtuO/m7DzJmQEqwQzEmoK4e3I4nx/Rm7rpCxr4yn4K9Ti3G/uISdh84HOToQotdGyoEWG2FCWeXp6XQOCGGGydlMPCxL48Z//xV/RnRu2UQIgsttmdRx+05eJjPVzq1FXHRVlthwtO53Zrz7Lh+lY67/d3FAY4mNFmyqOOstsIYx8/6tOIvI3sAcEH35vxiWPsj46wm4+RZsqjj0jPzrLbCGNd1Q9pxYY/mfLN2Gz/v1+rI8Ndmrw9iVKHBkkUdll2wj8ycnVZbYYxLRPjb6N4k149lwntLuP3sjgA8+un3QY6s7rNkUYdZbYUxx2oYH8PT4/qRu6OI3B0HjgzP2rYviFHVfZYs6qjy2oozrLbCmGMMTG3M3ed1YdqyzZzb1TlL8KVv1wU5qrrNkkUd9VNthR3YNqYyt5/diVM7NGbuukLO796c9Mw8vlm7Ldhh1VmWLOqoqZl5JMVFcV635sEOxZhaKTJC+NeV/YiLjmD99v2kNonnd+nL2XvQivVOhF+ThYgMF5G1IpIlIvdXMv56ESkQkaXu45ce40o9hk/zZ5x1zZ6Dh/l81RZG9rXaCmOOp0WDOP5+eR+ytu0jMS6abXuL+WR5frDDqpP8lixEJBJ4HrgI6A6ME5HulTR9X1X7uo+JHsMPeAwf6a8466Lpy/M5eLjMLu9hjA/O7dacG4alsmLTbgA+XJwX5IjqJn/uWQwCslQ1W1UPAVOAUX58v7CRnplHp2b16WO1Fcb45P6LutKjVRIAizbsJHdHUZAjqnt8ShYiEisiV4nI70XkwfKHl8laA7ker/PcYRWNFpHlIpIuIp7/KseJSIaIzBeRn1cR181um4yCggJfFqXOW799PxlWW2FMtcRGRR51OZBb384ka9veIEZU9/i6Z/EfnL2CEmC/x+N4Kvslq1hz/18gVVV7A18CkzzGtVXVNOAq4F8i0vGYmam+oqppqpqWnJzs25LUcVMzrbbCmBPRIbk+f7+8DwCrNu/h4mdm8+p32Xa3PR/5etXZNqo6vJrzzgM89xTaAJs9G6hqocfLV4G/eYzb7P7NFpGZQD8grE+ULi1Tpi7O44wuyTS32gpjqm10/9bM/rGAj5duJj4mksemf8+keRu4/exOXJmWQkSE7a1Xxdc9i7ki0qua814EdBaR9iISA4wFjjqrSUQ8rxs8EvjeHd5IRGLd502BYcDqar5/yJm3rtBqK4w5CSLCo5f2IrVJPPWiI/nXlX1pWj+WBz5cwWPTv7cLDh6Hr8niNCDTPQ12uYisEJHlx5tAVUuAO4AZOEngA1VdJSIPi0j52U13isgqEVkG3Alc7w7vBmS4w78BHlfVsE8W6Zm5VlthzEmqHxvFs+P6s31fMZ+uyOcNSD/TAAAbYklEQVSjXw3lhmGpvDZ7PS99mx3s8GotX7uhLjqRmavqdGB6hWEPejx/AHigkunmAtXdkwlp5bUVYwa0sdoKY05SrzYN+N3wrjz66fe8NT+HP43ozo79h/jb52tokhDDFQPttPSKfEoWqpojIn2A091Bs1R1mf/CMhWV11aM7m9dUMbUhF8Ma8/srO08+un3pLVrzJNj+rCz6DD3f7icRgkxnN/d9uA9+Xrq7F3AO0Az9/G2iEzwZ2DmaOmZeXRMTqBvSsNgh2JMSIiIEP5+eR8a1ItmwnuLKSkr48Wr+9OrTUPueHcxC9fvCHaItYqvxyxuBAar6oNuN9KpwE3+C8t4+qm2IsVqK4ypQU3rx/KvK/uSvX0/f5m2moTYKN64fiCtG9XjxkmLWOlWfRvfk4UApR6vS6m8jsL4wYd23wpj/GZYp6b86qyOvJ+Ry7Rlm2mcEMNbNw4mMTaKK1+ex/9WbQl2iLWCr8niDWCBiDwkIg8B84HX/BaVOaKsTJmamcfpnZNp0cBqK4zxh7vP60L/tg35/Ycr2FhYROuG9fjwV8PokFyfm9/K5K/Tvw/74j2fkoWq/hO4AdgB7ARuUNV/+TMw45iXXchmq60wxq+iIyN4emw/RGDClCUcLi2jRYM4/n3rEK4e3JaXv8vmjTnhfR/v4yYLEUly/zYGNgBvA28BOe4w42fpmXkkxkXZmRnG+FlK43j+Nro3y3J38ff/rQUgLjqSxy7tRZ+UhmF/aXNvexbvun8zgQyPR/lr40d7Dx7ms5X5jOxj960wJhAu7tWScYPa8vK32Xz3w08XJx3eowXL83azedeB40wd2o6bLFT1Evdve1Xt4PFor6odAhNi+Jq+ovy+FdYFZUygPHhJd7o0r889Hyxl296DAFzUswUi8MePV4btsQtf6yyGiUiC+/waEfmniLT1b2jGaiuMCbx6MZE8d1V/9h4s4d4PllFWpqQ2TeDRn/fk6zXbuOGNRSxcvyPsriPl69lQLwJFbhX3b4EcnGMXxk82bN/Pog1WW2FMMHRpnsiDP+vOrB+388os53pRVw9uxyOjerA8bxdXvDyPmyZncOBQqZc5hQ5fk0WJOml0FPC0qj4NJPovLDPVaiuMCaqrBrXlop4t+PuMtSzZuBOAa4eksuD35/HARV35as02rn1tAbuLDgc50sDwNVnsFZEHgGuAT937a0f7L6zwZrUVxgSfiPD4Zb1pnhTHnVOWsOegkxTqxURyy5kdef6q/izP283lL8/l+/w9QY7W/3xNFlcCxcCNqroF5/aoT/otqjBXXlsx2g5sGxNUDeKjeWZcXzbvOsgfPlp51HGKi3u15I0bBrJj/yF+9uxsbn9nMR8syg3ZPQ1fi/K2qOo/VXWW+3qjqk72b2jhq7y24gKrrTAm6Aa0a8w953fhv8s28++MvKPGDevUlC9+fSZXDW5LRs4Ofjt1OSOenUXujqIgRes/crwj+iIyW1VPE5G9HH3/bAFUVZP8HaCv0tLSNCOj7pd+7D14mIGPfcll/dvwf5faLT2MqQ1Ky5RrX1vA4o07+WTCaXRqduwhW1Vlwfod3Pp2JgJ0bp5IywZx/GFEN5ol1t7uZBHJVNU0b+281Vmc5v5NVNUkj0dibUoUoeSzFVustsKYWiYyQnjqyr7Ex0Rxx7tLOHj42LOgRIRTOzThvZtOZXD7Jgjwv1VbufrVBRTuKw580DXM1zqLU0Uk0eN1fREZ7L+wwld6Zh4dkhPoZ7UVxtQqzZPi+MflfVizZS//N/37Ktt1a5nES9cO4P1bhvD69QPJ3VnE6Bfnsn77/gBGW/OqU2exz+N1kTvM1KAN2/ezcMMOxgxoY7UVxtRCZ3dtxo2ntWfyvBxm+HDp8iEdm/DOLwez+8Bh7pqyhNKyulvI5/P9LNTj4IaqluH7/buNj8rvW3FZP+uCMqa2+u3wU+jVugG/TV/u07WiBrRrzEMje7A8bzcjnpnFF6u3VtqNVdv5miyyReROEYl2H3cB2f4MLNyUlSlTF2/iNKutMKZWi42K5Nlx/SgpLePuKUsp8eFaUaP6tua5q/pRdKiUmyZn0O3Bz0l79AveWZATgIhrhq/J4lZgKLAJyAMGAzf7K6hwND+7kE27DtiBbWPqgNSmCTx6aU8WbtjBM19n+TTNJb1b8eU9ZzLxujTuPKczHZrW58H/rKozp9n6WmexTVXHqmozVW2uqlep6jZ/BxdOrLbCmLrl0n5tuKx/a577+kfmZxf6NE1MVATndW/Or8/vwjPj+hEpwlUT5/PF6q1+jvbk+Xo2VBcR+UpEVrqve4vIH/0bWvjYe/Aw01fm8zO7b4Uxdcojo3rSrkkCd09Zys79h6o1bYsGcbx2fRrRkRE88OFyP0VYc3zthnoVeAA4DKCqy4Gx/goq3FhthTF1U0JsFM+O60fh/mLuS19W7cuWn945mWtPbcf2fYeYtmyzn6KsGb4mi3hVXVhhWElNBxOurLbCmLqrZ+sGPHBRN778fhuT5m6o9vTjBrVlUGpjfpu+jN0Hau91pXxNFttFpCPuJT9EZAzg9Ya0IjJcRNaKSJaI3F/J+OtFpEBElrqPX3qMGy8iP7qP8T7GWefkFDq1FaP7W22FMXXVDcNSObdrM/5v+hpWbtpdrWnjoiP5w4huHDxcxrNf/UhZLa3F8DVZ3A68DHQVkU3A3ThnSFXJvYz588BFQHdgnIh0r6Tp+6ra131MdKdtDPwZ56yrQcCfRaSRj7HWKVMXb0IELutv960wpq4SEZ68vA+NEqK5870l7C+uXsdL7zYNuKx/aybOXs/rc9b7KcqT4zVZiEgEkKaq5wHJQFdVPU1VvZ0gPAjIUtVsVT0ETMG5eZIvLgS+UNUdqroT+AIY7uO0dUb5fStO69SUlg3qBTscY8xJaJwQw1NX9mV94X7+PG1VtaYVEf5xeR+GdGjCkzPWsiKvensngeA1WbjV2ne4z/er6l4f590ayPV4necOq2i0iCwXkXQRSanmtHXa/PVWW2FMKBnasSl3nN2J9Mw8/rN0U7WmFRGeHtuXqAjhiRlrat2lQXzthvpCRH4jIiki0rj84WWayjrgKy79f4FUVe0NfAlMqsa0iMjNIpIhIhkFBQXelqHWSc/MIzE2igt7tAh2KMaYGnLXuZ1Ja9eIP3y0kpzC6l08sFlSHDee3oFZP25nae5OP0V4YnxNFr8AfgV8C2R4PI4nD0jxeN0GOOrcMFUtVNXya/e+CgzwdVp3+ldUNU1V05KTk31clNphX3EJn63YwiVWW2FMSImKjODpcf2IEJjw3hIOlXi/HIinkX1aArBuW+26Sq2vyaI7zsHqZcBS4Fmgh5dpFgGdRaS9iMTg1GVM82wgIi09Xo4Eyq/7OwO4QEQauQe2L3CHhYzpK/I5cLjUuqCMCUGtG9bjiTF9WJ63mydnrKnWtB2T69MiKY7J8zdQXFJ7Ljjoa7KYBHQDnsFJFN34qcuoUqpagnOsYwZOEvhAVVeJyMMiMtJtdqeIrBKRZcCdwPXutDuAR3ASziLgYXdYyEjPzKND0wT6t7XaCmNC0fCeLbjm1La8Oms936z1/epIIsJfRvVg5aY9nPfPb7nn/aXsKqpedbg/HPe2qkcaiSxT1T7ehgVTXbqtak7hfs58cib3XXgKt5/dKdjhGGP85ODhUn7+/BwK9hbz2V2n0yzJ9ytKv7Mgh2/WbOPbHwpoWj+Wt24cTKdm9Ws8xhq5raqHJSJyqsfMBwNzTjS4cGe1FcaEh7ho53Lm+w+VcM8Hy6pVcHf14HZMHD+QD28bRuG+Q3yQket9Ij/yNVkMBuaKyAYR2QDMA84UkRUiUvuvgFWLWG2FMeGlc/NEHvpZD2Znbeel79ZVe/pebRrQsmEcW3Yf9EN0vvP1bnchVxAXLOW1Fb8dfkqwQzHGBMiVA1OYlbWdf/zvBwa3b8KAdtW7IEWj+JigJwtf72eRc7yHv4MMJVZbYUz4ERH+elkvWjaI4873llT7goGtGsaxM8gHuX3thjI1YH9xCZ+v3MIlfVpabYUxYSYpLppnxvVjy56D/P7DFdW+nPmP2/bx3Q/BKz62ZBFA01fkU3TIaiuMCVf92zbi3gu68OmKfKYs8v2A9dWD2wHw2uzgXWTQkkUApWfm0b5pAv3bhuQFdI0xPrj1jI6c1qkpf/nvKn7c6tul9oZ1asqFPZqTvX2fn6OrmiWLANlYWMSC9TsYM8DuW2FMOIuIEP55ZR/qx0Zxx7tLOHjYtyrtgamNyd1xgBdmZvk5wspZsgiQqYvzEIFL+1lthTHhrlliHP+4oi9rt+7lkU9W+zTN2EFt6dK8PpPmbqj28Y6aYMkiAMrKlKmLndqKVg2ttsIYA2d2SebmMzrwzoKNfLbC641HqR8bRefmiWzdU8xnK7cEIMKjWbIIgAXrd5C30+5bYYw52m8uOIU+bRrwu6nLydtZ5LX9hHOcywMtzgn85cstWQRAeW3FBd2ttsIY85OYqAieGdePMoW7pyylpPT4lzPv2iKJDskJLMvbFaAIf2LJws/2F5fw2cp8LunTknoxVlthjDlauyYJPHZpTzJydvL0Vz96bX9G52SW5Qb+tquWLPzMaiuMMd6M6tuaywe04blvspi7bvtx27ZsEMeh0jKmLNwYoOgcliz8zGorjDG++MuoHrRvmsDdU5ZSuK+4ynbXDUmlXnQk6Zl5AYzOkoVf5e6w2gpjjG/iY6J4dlw/dhUd5r705VWeHlsvJpLzuzcnI2cnew9W7xpTJ8OShR9ZbYUxpjp6tGrA7y/uytdrtvH6nA1VtktOjAXgwf+sClBkliz8pry2YlhHq60wxvhu/NBUzuvWnMc/+56Vmyo/kP2Hi7tRLzqSjTu8n25bUyxZ+MnCDTvI3WG1FcaY6hERnhzTmyYJsUx4bwn7ikuOaRMRIdW+J8bJsmThJ+mZedS3+1YYY05Ao4QYnh7bl5zC/Tz48cpK2yhKZs7OgF36w5KFH+wvLmH6inwu6W21FcaYEzO4QxMmnNOZD5ds4sPFx575VH7nvLydBwISjyULP/hs5RarrTDGnLQJ53RiUGpj/vjxStZv33/UuNvPdi79UVpmexZ1VnpmLqlN4gPep2iMCS1RkRH8a2xfoiMjmPDeYopLfrqceYmbJBZu2BGQWCxZ1LDcHUXMz7baCmNMzWjVsB5PjunNyk17eOLztUeGj+zTCoA1+b7dQOlkWbKoYUdqK/pbF5QxpmZc0KMF44e047XZ6/l6zVYA4qIj6dSsPusKAnP3PEsWNciztqK11VYYY2rQAxd3o1vLJH7z7+Vs3eMc3C4uKeXbHwoC8v6WLGqQ1VYYY/wlLjqSZ8f148ChUu6espTSMiUxNhrA51uzngy/JgsRGS4ia0UkS0TuP067MSKiIpLmvk4VkQMistR9vOTPOGuK1VYYY/ypU7P6/GVkD+ZlF/LizCwu6+9cSmh1/h6/v7ffkoWIRALPAxcB3YFxItK9knaJwJ3Aggqj1qlqX/dxq7/irClWW2GMCYTL09rwsz6teOrLH4/sUSxa7/8zovy5ZzEIyFLVbFU9BEwBRlXS7hHgCeCgH2Pxu8/d2orR1gVljPEjEeGxS3vSumE9nv06C4BDJce/w15N8GeyaA3kerzOc4cdISL9gBRV/aSS6duLyBIR+VZETq/sDUTkZhHJEJGMgoLAHOSpSnpmHu2axJNmtRXGGD9LiovmmXH9jhTkvbPA/zdC8meyqKzI4EipoYhEAE8B91bSLh9oq6r9gHuAd0Uk6ZiZqb6iqmmqmpacnFxDYVdf7o4i5mUXMqa/1VYYYwKjb0pD7rvwFAC27PF/x0yUH+edB6R4vG4DbPZ4nQj0BGa6P7AtgGkiMlJVM4BiAFXNFJF1QBcgw4/xnrAPF29CBC6zLihjTADddHoHZqzaQsfk+n5/L38mi0VAZxFpD2wCxgJXlY9U1d1A0/LXIjIT+I2qZohIMrBDVUtFpAPQGcj2Y6wnrKxMSV+cy9COTay2whgTUBERwtTbhgakR8Nv3VCqWgLcAcwAvgc+UNVVIvKwiIz0MvkZwHIRWQakA7eqamAugFJNi6y2whgTRIHq+vbnngWqOh2YXmHYg1W0Pcvj+VRgqj9jqylWW2GMCQdWwX0S9heX8OmKfEb0akl8jF/zrjHGBJUli5NQXlsxJs26oIwxoc2SxUmw2gpjTLiwZHGCrLbCGBNOLFmcoI+WWG2FMSZ8WLI4AapKemae1VYYY8KGJYsTsGjDTjbuKGK03Q3PGBMmLFmcgPTMXBJiIhne02orjDHhwZJFNRUdKuHT5fmM6G21FcaY8GHJopo+X7mF/YdKGTMgxXtjY4wJEZYsqik9M4+2jeMZmGq1FcaY8GHJohrydhYxd10hYwZYbYUxJrxYsqiGDxdvAjhyk3RjjAkXlix85Flb0aZRfLDDMcaYgLJk4aPy2gq7b4UxJhxZsvCR1VYYY8KZJQsfFB0qYfqKLVZbYYwJW5YsfDBj1Rb2FZdYbYUxJmxZsvCB1VYYY8KdJQsvymsrRtt9K4wxYcyShRcfLd6EqtVWGGPCmyWL41BV0hfnMaRDE1IaW22FMSZ8WbI4joycneQUWm2FMcZYsjiO9Iw8EmIiuaiX1VYYY8KbJYsqFB0q4dMV+Vzcy2orjDHGkkUVfqqtsC4oY4zxa7IQkeEislZEskTk/uO0GyMiKiJpHsMecKdbKyIX+jPOyvxUW9E40G9tjDG1jt+ShYhEAs8DFwHdgXEi0r2SdonAncACj2HdgbFAD2A48II7v4DYtOvAkdqKiAirrTDGGH/uWQwCslQ1W1UPAVOAUZW0ewR4AjjoMWwUMEVVi1V1PZDlzi8gPlqcZ7UVxhjjwZ/JojWQ6/E6zx12hIj0A1JU9ZPqTusv5fetsNoKY4z5iT+TRWX9N3pkpEgE8BRwb3Wn9ZjHzSKSISIZBQUFJxyop8ycnWyw2gpjjDmKP5NFHuB5mdY2wGaP14lAT2CmiGwATgWmuQe5vU0LgKq+oqppqpqWnJxcI0GnZ1pthTHGVOTPZLEI6Cwi7UUkBueA9bTykaq6W1WbqmqqqqYC84GRqprhthsrIrEi0h7oDCz0Y6wAHDhUyifL87nIaiuMMeYofvtFVNUSEbkDmAFEAq+r6ioReRjIUNVpx5l2lYh8AKwGSoDbVbXUX7GWs9oKY4ypnF//fVbV6cD0CsMerKLtWRVePwY85rfgKpGemUdK43oMstoKY4w5ilVwuzbtOsCcdduttsIYYyphycJVXlsxur91QRljTEWWLPiptuLUDo2ttsIYYyphyQLP2ooU742NMSYMWbLAObAdHxPJRT2ttsIYYyoT9sniwKFSPl3u3LciIdZqK4wxpjJhnyz2HDzMWV2bcUWadUEZY0xVwv5f6eZJcTw7rl+wwzDGmFot7PcsjDHGeGfJwhhjjFeWLIwxxnhlycIYY4xXliyMMcZ4ZcnCGGOMV5YsjDHGeGXJwhhjjFeiqsGOoUaISAGQ4zGoKbA9SOEEWzgvO4T38tuyh68TXf52qprsrVHIJIuKRCRDVdOCHUcwhPOyQ3gvvy17eC47+H/5rRvKGGOMV5YsjDHGeBXKyeKVYAcQROG87BDey2/LHr78uvwhe8zCGGNMzQnlPQtjjDE1xJKFMcYYr+p8shCR4SKyVkSyROT+SsbHisj77vgFIpIa+Cj9w4dlv0dEVovIchH5SkTaBSNOf/G2/B7txoiIikjInFbpy7KLyBXu579KRN4NdIz+4sN231ZEvhGRJe62f3Ew4vQHEXldRLaJyMoqxouIPOOum+Ui0r/G3lxV6+wDiATWAR2AGGAZ0L1Cm18BL7nPxwLvBzvuAC772UC8+/y2UFl2X5ffbZcIfAfMB9KCHXcAP/vOwBKgkfu6WbDjDuCyvwLc5j7vDmwIdtw1uPxnAP2BlVWMvxj4DBDgVGBBTb13Xd+zGARkqWq2qh4CpgCjKrQZBUxyn6cD54qIBDBGf/G67Kr6jaoWuS/nA20CHKM/+fLZAzwCPAEcDGRwfubLst8EPK+qOwFUdVuAY/QXX5ZdgST3eQNgcwDj8ytV/Q7YcZwmo4DJ6pgPNBSRljXx3nU9WbQGcj1e57nDKm2jqiXAbqBJQKLzL1+W3dONOP9xhAqvyy8i/YAUVf0kkIEFgC+ffRegi4jMEZH5IjI8YNH5ly/L/hBwjYjkAdOBCYEJrVao7u+Cz6JqYiZBVNkeQsVzgX1pUxf5vFwicg2QBpzp14gC67jLLyIRwFPA9YEKKIB8+eyjcLqizsLZo5wlIj1VdZefY/M3X5Z9HPCmqv5DRIYAb7nLXub/8ILOb793dX3PIg9I8XjdhmN3OY+0EZEonN3S4+3G1RW+LDsich7wB2CkqhYHKLZA8Lb8iUBPYKaIbMDpv50WIge5fd3u/6Oqh1V1PbAWJ3nUdb4s+43ABwCqOg+Iw7nIXjjw6XfhRNT1ZLEI6Cwi7UUkBucA9rQKbaYB493nY4Cv1T0SVMd5XXa3G+ZlnEQRKn3W5Y67/Kq6W1WbqmqqqqbiHLMZqaoZwQm3Rvmy3X+Mc4IDItIUp1sqO6BR+ocvy74ROBdARLrhJIuCgEYZPNOA69yzok4Fdqtqfk3MuE53Q6lqiYjcAczAOUvidVVdJSIPAxmqOg14DWc3NAtnj2Js8CKuOT4u+5NAfeDf7jH9jao6MmhB1yAflz8k+bjsM4ALRGQ1UArcp6qFwYu6Zvi47PcCr4rIr3G6YK4PkX8QEZH3cLoWm7rHZP4MRAOo6ks4x2guBrKAIuCGGnvvEFmHxhhj/Kiud0MZY4wJAEsWxhhjvLJkYYwxxitLFsYYY7yyZGGMMcYrSxbG1AIiklp+JVEROUtEQu0SJaaOs2RhzElwi5/se2RCnm3kxlSTuxfwvYi8ACwGrhWReSKyWET+LSL13XYDRWSuiCwTkYUikuhOO8ttu1hEhgZ3aYzxjSULY07MKcBk4HycaxGdp6r9gQzgHvdSFO8Dd6lqH+A84ACwDTjfbXsl8Ewwgjemuur05T6MCaIcVZ0vIpfg3GBnjntJlRhgHk4yyVfVRQCqugdARBKA50SkL85lOLoEI3hjqsuShTEnZr/7V4AvVHWc50gR6U3ll4b+NbAV6IOzZx9KN2UyIcy6oYw5OfOBYSLSCUBE4kWkC7AGaCUiA93hiR6XyM93761wLc7F8Iyp9SxZGHMSVLUA5wZL74nIcpzk0dW95eeVwLMisgz4AudS2S8A40VkPk4X1P5KZ2xMLWNXnTXGGOOV7VkYY4zxypKFMcYYryxZGGOM8cqShTHGGK8sWRhjjPHKkoUxxhivLFkYY4zx6v8ButpLsJurWr0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold is 19900000\n",
      "For the threshold [precision, recall, f_score]:\n",
      " train - 0.566 0.816 0.668\n",
      " dev - 0.557 0.844 0.671\n"
     ]
    }
   ],
   "source": [
    "def word_frequency_threshold_refined(training_file, development_file, counts):\n",
    "    threshold = -1\n",
    "    best_fscore = 0.0\n",
    "    best_training_peformance = []\n",
    "    xs = []\n",
    "    ys = []\n",
    "    upper_bound = 200000000\n",
    "    lower_bound = 0\n",
    "    for gap in range(6, 2, -1):\n",
    "        for word_freq in range(lower_bound, upper_bound, 10**gap):\n",
    "            training_performance = word_frequency_classifier(training_file, word_freq)\n",
    "            if best_fscore < training_performance[2]:\n",
    "                best_fscore = training_performance[2]\n",
    "                threshold = word_freq\n",
    "                best_training_peformance = training_performance\n",
    "            xs.append(training_performance[1])\n",
    "            ys.append(training_performance[0])\n",
    "        lower_bound = threshold - 10**gap\n",
    "        upper_bound = threshold + 10**gap\n",
    "    \n",
    "    plt.plot(xs, ys)\n",
    "    plt.xlabel('recall')\n",
    "    plt.ylabel('precision')\n",
    "    plt.title('percision and recall as the word frequency grows')\n",
    "    plt.show()\n",
    "    print (\"Threshold is {}\".format(threshold))\n",
    "    \n",
    "    development_performance = word_frequency_classifier(development_file, threshold)\n",
    "\n",
    "    return best_training_peformance, development_performance\n",
    "\n",
    "counts = load_ngram_counts('../data/ngram_counts.txt.gz')\n",
    "best_train_perf, dev_perf = word_frequency_threshold_refined(\"../data/complex_words_training.txt\",\\\n",
    "                                                     \"../data/complex_words_development.txt\", counts)\n",
    "print (\"For the threshold [precision, recall, f_score]:\\n train - {0:.3f} {1:.3f} {2:.3f}\\n dev - {3:.3f} {4:.3f} {5:.3f}\"\\\n",
    "       .format(best_train_perf[0], best_train_perf[1], best_train_perf[2], dev_perf[0], dev_perf[1], dev_perf[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our former search was pretty good. The straight line remained since it's not where the refinement took place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the word lengths classifier achieved better results than the word frequencies classifier. Looks that the comment above about how this frequencies are very close to indicators about word complexity didn't stand in reality.\n",
    "Note: the upper line in the right side of the function is caused by the many dots we get in the area of the refinement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1: Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.4950379451255108, 0.9797804737146159, 0.6577467519875897], [0.46929316338354576, 0.9688995215311005, 0.6323185011709602])\n"
     ]
    }
   ],
   "source": [
    "def naive_bayes(training_file, development_file, counts):\n",
    "    twords, tlabels = load_file(training_file)\n",
    "    word_lengths = [len(w) for w in twords]\n",
    "    word_counts = [counts[w] for w in twords]\n",
    "    lmean, lsd = get_list_mean_sd(word_lengths)\n",
    "    cmean, csd = get_list_mean_sd(word_counts)\n",
    "\n",
    "    # training the model\n",
    "    X_train=[((len(w)-lmean)/lsd, (counts[w]-cmean)/csd) for w in twords]\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, tlabels)\n",
    "    \n",
    "    dwords, dlabels = load_file(development_file)\n",
    "    X_dev=[((len(w)-lmean)/lsd, (counts[w]-cmean)/csd) for w in dwords]\n",
    "    \n",
    "    # prediction\n",
    "    training_performance = get_all_metrics(clf.predict(X_train), tlabels)\n",
    "    development_performance = get_all_metrics(clf.predict(X_dev), dlabels)\n",
    "    return training_performance, development_performance\n",
    "\n",
    "def get_list_mean_sd(lst):\n",
    "    return np.average(lst), np.std(lst)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_file = \"../data/complex_words_training.txt\"\n",
    "    development_file = \"../data/complex_words_development.txt\"\n",
    "    \n",
    "    counts = load_ngram_counts('../data/ngram_counts.txt.gz')\n",
    "    \n",
    "    print(naive_bayes(training_file, development_file, counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4: Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll simply try to find words which were labeled both 'simple' and 'complex', and the reason to the difference is likely to be the different contexts, which give the word a different meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: campaign\n",
      "Simple Context: `` Given the way the meeting transpired , '' Ray Halbritter , an Oneida representative and leader of the `` Change the Mascot Campaign , '' said Wednesday , `` it became somewhat evident they were defending the continued use of the name .\n",
      "Complex Context: The campaign around the Maine Bear Hunting Ban Initiative , which will appear on the ballot as Question 1 , has shed light on Maine 's unique status in the wildlife world .\n",
      "\n",
      "Word: element\n",
      "Simple Context: Element 118 , for example , is the heaviest element to date , with 118 protons alongside 176 neutrons .\n",
      "Complex Context: Arturo Sanchez is in his element .\n",
      "\n",
      "Word: inspired\n",
      "Simple Context: Inspired by their courage , demonstrators across the South adopted their `` jail not bail '' tactic and filled jail cells .\n",
      "Complex Context: Miguel Castaeda started Tech Connect in 2009 , inspired by a similar program in San Francisco .\n",
      "\n",
      "Word: district\n",
      "Simple Context: Before the jurors began their deliberations , District Judge George A. O'Toole Jr. reminded them that they had to decide if Tsarnaev would spend the rest of his life in prison or if he should be executed by the federal government .\n",
      "Complex Context: Now 80 and a resident of the Park La Brea complex in the Fairfax district , Prochnik uses that vat of chocolate as a centerpiece during talks about the Holocaust that he gives to schoolchildren .\n",
      "\n",
      "Word: commissioner\n",
      "Simple Context: NFL Commissioner Roger Goodell said he issued the penalties based on `` the critical importance of protecting the integrity of the game . ''\n",
      "Complex Context: `` As we have indicated earlier , another lawsuit of this type is not unexpected , '' NHL Deputy Commissioner Bill Daly said in an email to The Associated Press .\n",
      "\n",
      "Word: opponents\n",
      "Simple Context: Opponents led by a group called Save Maine 's Bear Hunt say the Humane Society 's involvement shows that this is a fight being bankrolled by outside lobby groups with a broader agenda of ending sport hunting nationwide .\n",
      "Complex Context: `` I love droppin ' 'em , '' she says about opponents .\n",
      "\n",
      "Word: sprouts\n",
      "Simple Context: The scientists were initially skeptical about what they would find on the construction site near the new Sprouts Farmers Market less than 100 yards from busy Matlock Road .\n",
      "Complex Context: An apple can kill , a sprinkle of sprouts can send you to the hospital and your succulent , pan-seared red snapper may actually be tilefish .\n",
      "\n",
      "Word: league\n",
      "Simple Context: One piece of that market , e-sport multiplayer video-gaming competitions , is expected to bring in $ 612 million this year for games such as Dota 2 and League of Legends .\n",
      "Complex Context: Tom Brady has been an NFL golden boy , with movie-star looks , a supermodel wife , four Super Bowl championship rings , and a regular-guy-makes-good back story -- he went from being a sixth-round draft pick to one of the greatest quarterbacks in league history .\n",
      "\n",
      "Word: hoverboard\n",
      "Simple Context: `` Hoverboard is on fire , '' he says in a video taken right after he jumped off the burning piece of equipment .\n",
      "Complex Context: Timothy Cade had only owned his hoverboard for three days when it exploded beneath him .\n",
      "\n",
      "Word: slavery\n",
      "Simple Context: Slavery was a `` side issue to the Civil War , '' said Pat Hardy , a Republican board member , when the board adopted the standards in 2010 .\n",
      "Complex Context: And when it comes to the Civil War , children are supposed to learn that the conflict was caused by `` sectionalism , states ' rights and slavery '' -- written deliberately in that order to telegraph slavery 's secondary role in driving the conflict , according to some members of the State Board of Education .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_file_with_sentences(data_file):\n",
    "    words = []\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "                sentences.append(line_split[3])\n",
    "            i += 1\n",
    "    return words, labels, sentences\n",
    "\n",
    "def context_sensitive(file):\n",
    "    words, labels, sentences = load_file_with_sentences(file)\n",
    "    complex_sentences = []\n",
    "    simple_sentences = []\n",
    "    ambigious_words = []\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i+1, len(words)):\n",
    "            if words[i] == words[j] and not labels[i] == labels[j]:\n",
    "                ambigious_words.append(words[i])\n",
    "                if labels[i] == 1:\n",
    "                    complex_sentences.append(sentences[i])\n",
    "                    simple_sentences.append(sentences[j])\n",
    "                else:\n",
    "                    complex_sentences.append(sentences[j])\n",
    "                    simple_sentences.append(sentences[i])\n",
    "    return ambigious_words, simple_sentences, complex_sentences\n",
    "\n",
    "w, s, c = context_sensitive(\"../data/complex_words_training.txt\")\n",
    "for i in range(10):\n",
    "    print('Word: ' + w[i])\n",
    "    print('Simple Context: ' + s[i])\n",
    "    print('Complex Context: ' + c[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our approach was correct in its core - in the second and seventh sentences (words 'element' and 'sprouts', appropriately), the observed word has a different meaning. It's unclear why the other words got different labels: a possible reason might belong with the experiment process - the annotators had to point at 'complex' words, and in some sentences, even the most 'complex' words were simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Reuters Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is a copy of the code intended for initializing the dataset (which we were referred to in the homework page). Can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Eustache Diemert <eustache@diemert.fr>\n",
    "#          @FedericoV <https://github.com/FedericoV/>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from glob import glob\n",
    "import itertools\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn.externals.six.moves import html_parser\n",
    "from sklearn.externals.six.moves.urllib.request import urlretrieve\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def _not_in_sphinx():\n",
    "    # Hack to detect whether we are running by the sphinx builder\n",
    "    return '__file__' in globals()\n",
    "class ReutersParser(html_parser.HTMLParser):\n",
    "    \"\"\"Utility class to parse a SGML file and yield documents one at a time.\"\"\"\n",
    "\n",
    "    def __init__(self, encoding='latin-1'):\n",
    "        html_parser.HTMLParser.__init__(self)\n",
    "        self._reset()\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        method = 'start_' + tag\n",
    "        getattr(self, method, lambda x: None)(attrs)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        method = 'end_' + tag\n",
    "        getattr(self, method, lambda: None)()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.in_title = 0\n",
    "        self.in_body = 0\n",
    "        self.in_topics = 0\n",
    "        self.in_topic_d = 0\n",
    "        self.title = \"\"\n",
    "        self.body = \"\"\n",
    "        self.topics = []\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "    def parse(self, fd):\n",
    "        self.docs = []\n",
    "        for chunk in fd:\n",
    "            self.feed(chunk.decode(self.encoding))\n",
    "            for doc in self.docs:\n",
    "                yield doc\n",
    "            self.docs = []\n",
    "        self.close()\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.in_body:\n",
    "            self.body += data\n",
    "        elif self.in_title:\n",
    "            self.title += data\n",
    "        elif self.in_topic_d:\n",
    "            self.topic_d += data\n",
    "\n",
    "    def start_reuters(self, attributes):\n",
    "        pass\n",
    "\n",
    "    def end_reuters(self):\n",
    "        self.body = re.sub(r'\\s+', r' ', self.body)\n",
    "        self.docs.append({'title': self.title,\n",
    "                          'body': self.body,\n",
    "                          'topics': self.topics})\n",
    "        self._reset()\n",
    "\n",
    "    def start_title(self, attributes):\n",
    "        self.in_title = 1\n",
    "\n",
    "    def end_title(self):\n",
    "        self.in_title = 0\n",
    "\n",
    "    def start_body(self, attributes):\n",
    "        self.in_body = 1\n",
    "\n",
    "    def end_body(self):\n",
    "        self.in_body = 0\n",
    "\n",
    "    def start_topics(self, attributes):\n",
    "        self.in_topics = 1\n",
    "\n",
    "    def end_topics(self):\n",
    "        self.in_topics = 0\n",
    "\n",
    "    def start_d(self, attributes):\n",
    "        self.in_topic_d = 1\n",
    "\n",
    "    def end_d(self):\n",
    "        self.in_topic_d = 0\n",
    "        self.topics.append(self.topic_d)\n",
    "        self.topic_d = \"\"\n",
    "\n",
    "\n",
    "def stream_reuters_documents(data_path=None):\n",
    "    \"\"\"Iterate over documents of the Reuters dataset.\n",
    "\n",
    "    The Reuters archive will automatically be downloaded and uncompressed if\n",
    "    the `data_path` directory does not exist.\n",
    "\n",
    "    Documents are represented as dictionaries with 'body' (str),\n",
    "    'title' (str), 'topics' (list(str)) keys.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                    'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "\n",
    "    if data_path is None:\n",
    "        data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "            if _not_in_sphinx():\n",
    "                sys.stdout.write(\n",
    "                    '\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb))\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                    reporthook=progress)\n",
    "        if _not_in_sphinx():\n",
    "            sys.stdout.write('\\r')\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "\n",
    "    parser = ReutersParser()\n",
    "    for filename in glob(os.path.join(data_path, \"*.sgm\")):\n",
    "        for doc in parser.parse(open(filename, 'rb')):\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 + 2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the required information with slight manipulations of the code we were referred to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents is 19716\n",
      "Number of categories is 445\n",
      "Mean of documents per category is 89.87, standard deviation is 643.93, min is 1, max is 12542\n",
      "Number of words is 2638722\n",
      "Number of characters is 13343784\n"
     ]
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n",
    "                               alternate_sign=False)\n",
    "\n",
    "\n",
    "# Iterator over parsed Reuters SGML files.\n",
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "# Here are some classifiers that support the `partial_fit` method\n",
    "partial_fit_classifiers = {\n",
    "    'SGD': SGDClassifier(max_iter=5),\n",
    "    'Perceptron': Perceptron(tol=1e-3),\n",
    "    'NB Multinomial': MultinomialNB(alpha=0.01),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(tol=1e-3),\n",
    "}\n",
    "\n",
    "categories_dict = {}\n",
    "def get_minibatch(doc_iter, size, pos_class=''):\n",
    "    \"\"\"Extract a minibatch of examples, return a tuple X_text, y.\n",
    "\n",
    "    Note: size is before excluding invalid docs with no topics assigned.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for doc in itertools.islice(doc_iter, size):\n",
    "        for topic in doc['topics']:\n",
    "            categories_dict[topic] = (categories_dict[topic] + 1) if topic in categories_dict else 1\n",
    "        if doc['topics']:\n",
    "            data.append((u'{title}\\n\\n{body}'.format(**doc), pos_class in doc['topics']))\n",
    "    if not len(data):\n",
    "        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n",
    "    X_text, y = zip(*data)\n",
    "    return X_text, np.asarray(y, dtype=int)\n",
    "\n",
    "\n",
    "def iter_minibatches(doc_iter, minibatch_size):\n",
    "    \"\"\"Generator of minibatches.\"\"\"\n",
    "    X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "    while len(X_text):\n",
    "        yield X_text, y\n",
    "        X_text, y = get_minibatch(doc_iter, minibatch_size)\n",
    "\n",
    "# We will feed the classifier with mini-batches of 1000 documents; this means\n",
    "# we have at most 1000 docs in memory at any time.  The smaller the document\n",
    "# batch, the bigger the relative overhead of the partial fit methods.\n",
    "minibatch_size = 1000\n",
    "\n",
    "# Create the data_stream that parses Reuters SGML files and iterates on\n",
    "# documents as a stream.\n",
    "minibatch_iterators = iter_minibatches(data_stream, minibatch_size)\n",
    "total_vect_time = 0.0\n",
    "\n",
    "num_docs = 0\n",
    "total_chars = 0\n",
    "total_words = 0\n",
    "num_docs = 0\n",
    "# Main loop : iterate on mini-batches of examples\n",
    "for i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n",
    "\n",
    "    X_train = vectorizer.transform(X_train_text)\n",
    "    num_chars = 0\n",
    "    num_words = 0\n",
    "    for line in X_train_text:\n",
    "            words = line.split()\n",
    "            num_words += len(words)\n",
    "            for w in words:\n",
    "                num_chars += len(w)\n",
    "    total_chars += num_chars\n",
    "    total_words += num_words\n",
    "    num_docs += X_train.shape[0]\n",
    "\n",
    "print (\"Number of documents is {}\".format(num_docs))\n",
    "print (\"Number of categories is {}\".format(len(categories_dict)))\n",
    "\n",
    "categories_values = list(categories_dict.values())\n",
    "categories_mean, categories_sd = get_list_mean_sd(categories_values)\n",
    "\n",
    "print (\"Mean of documents per category is {}, standard deviation is {}, min is {}, max is {}\". \n",
    "      format(round(categories_mean, 2), round(categories_sd, 2), min(categories_dict.values()), max(categories_dict.values())))\n",
    "\n",
    "print (\"Number of words is {}\".format(total_words))\n",
    "print (\"Number of characters is {}\".format(total_chars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers use the method partial fit in order to learn the tranining in chuncks - if we don't want to hold all the dataset in the RAM or maybe we can't do it since we get the training set as a stream of data, we want to be able to learn from the accessible data, and later on to adjust our classifier according to received data. In that case, we use a classifier which has this possibility. For example, the SGD classifier builds a function of the desired params of the classifier we're looking for, and then try to minimize it according to the samples it gets, while it sample from the dataset stochastically, and adjusts the result according the specific sample. It's clear that it might take the samples from a stream. This is also useful in case that newer data is more relevant than old data, and then it's possible to control the adjusment rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hashing vectorizer takes a text document and converts it to a vector holding the counts of tokens in the text. It's being used under the assumption that there is a very small probability that 2 different samples will be converted to the same vector. Than the classifiers take the received vector as the sample to learn from (or predict on). It's important to acheive 'streaming classification' since there is no state computed during fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Spam Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the given notebook code. Can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "def progress(i, end_val, bar_length=50):\n",
    "    '''\n",
    "    Print a progress bar of the form: Percent: [#####      ]\n",
    "    i is the current progress value expected in a range [0..end_val]\n",
    "    bar_length is the width of the progress bar on the screen.\n",
    "    '''\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('../data/spam/spam',        SPAM),\n",
    "    ('../data/spam/easy_ham',    HAM),\n",
    "    ('../data/spam/hard_ham',    HAM),\n",
    "    ('../data/spam/beck-s',      HAM),\n",
    "    ('../data/spam/farmer-d',    HAM),\n",
    "    ('../data/spam/kaminski-v',  HAM),\n",
    "    ('../data/spam/kitchen-l',   HAM),\n",
    "    ('../data/spam/lokay-m',     HAM),\n",
    "    ('../data/spam/williams-w3', HAM),\n",
    "    ('../data/spam/BG',          SPAM),\n",
    "    ('../data/spam/GP',          SPAM),\n",
    "    ('../data/spam/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    '''\n",
    "    Generator of pairs (filename, filecontent)\n",
    "    for all files below path whose name is not in SKIP_FILES.\n",
    "    The content of the file is of the form:\n",
    "        header....\n",
    "        <emptyline>\n",
    "        body...\n",
    "    This skips the headers and returns body only.\n",
    "    '''\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "\n",
    "def build_data_frame(l, path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, (file_name, text) in enumerate(read_files(path)):\n",
    "        if ((i+l) % 100 == 0):\n",
    "            progress(i+l, 58910, 50)\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "   \n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame, len(rows)\n",
    "\n",
    "def load_data():\n",
    "    data = DataFrame({'text': [], 'class': []})\n",
    "    l = 0\n",
    "    for path, classification in SOURCES:\n",
    "        data_frame, nrows = build_data_frame(l, path, classification)\n",
    "        data = data.append(data_frame, sort=False)\n",
    "        l += nrows\n",
    "    data = data.reindex(numpy.random.permutation(data.index))\n",
    "    return data\n",
    "\n",
    "# This should take about 2 minutes\n",
    "data=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "        ('classifier',         MultinomialNB())\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def train(data = None, n_folds = 6):\n",
    "    if data is None:\n",
    "        print(\"Loading data...\")\n",
    "        data = load_data()\n",
    "        print(\"Data loaded\")\n",
    "    k_fold = KFold(n_splits = n_folds)\n",
    "    pipeline = build_pipeline()\n",
    "    scores = []\n",
    "    confusion = numpy.array([[0, 0], [0, 0]])\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold.split(data)):\n",
    "        train_text = data.iloc[train_indices]['text'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "        test_text = data.iloc[test_indices]['text'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "        print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        print(\"Testing for fold %d\" % i)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "\n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "        scores.append(score)\n",
    "        print(\"Score for %d: %2.2f\" % (i, score))\n",
    "        print(\"Confusion matrix for %d: \" % i)\n",
    "        print(confusion)\n",
    "\n",
    "    print('Total emails classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 6 folds\n",
      "Training for fold 0\n",
      "Testing for fold 0\n",
      "Score for 0: 0.98\n",
      "Confusion matrix for 0: \n",
      "[[3854   18]\n",
      " [ 267 5680]]\n",
      "Training for fold 1\n",
      "Testing for fold 1\n",
      "Score for 1: 0.98\n",
      "Confusion matrix for 1: \n",
      "[[ 7772    35]\n",
      " [  496 11335]]\n",
      "Training for fold 2\n",
      "Testing for fold 2\n",
      "Score for 2: 0.98\n",
      "Confusion matrix for 2: \n",
      "[[11711    50]\n",
      " [  741 16954]]\n",
      "Training for fold 3\n",
      "Testing for fold 3\n",
      "Score for 3: 0.98\n",
      "Confusion matrix for 3: \n",
      "[[15641    65]\n",
      " [  966 22602]]\n",
      "Training for fold 4\n",
      "Testing for fold 4\n",
      "Score for 4: 0.98\n",
      "Confusion matrix for 4: \n",
      "[[19501    74]\n",
      " [ 1201 28316]]\n",
      "Training for fold 5\n",
      "Testing for fold 5\n",
      "Score for 5: 0.98\n",
      "Confusion matrix for 5: \n",
      "[[23450    89]\n",
      " [ 1427 33944]]\n",
      "Total emails classified: 58910\n",
      "Score: 0.9781590913527961\n",
      "Confusion matrix:\n",
      "[[23450    89]\n",
      " [ 1427 33944]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we expect much more bigrams than unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = len(pipeline.get_params()['count_vectorizer'].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams is 615615 and number of bigrams is 2949080\n"
     ]
    }
   ],
   "source": [
    "bigrams_count = 0\n",
    "for feature_name in pipeline.get_params()['count_vectorizer'].get_feature_names():\n",
    "    if (' ' in feature_name):\n",
    "        bigrams_count +=1\n",
    "\n",
    "print (\"Number of unigrams is {} and number of bigrams is {}\".format(feature_number - bigrams_count, bigrams_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "all_indices = range(0, len(data))\n",
    "\n",
    "def print_50_freq_bi_uni(indices):\n",
    "\n",
    "    text = data.iloc[indices]['text'].values\n",
    "    y = data.iloc[indices]['class'].values.astype(str)\n",
    "    vec = cv.fit(text, y)\n",
    "    bag_of_words = vec.transform(text)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq_sorted =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    unigrams_count = 0\n",
    "    bigrams_count = 0\n",
    "    unigrams = []\n",
    "    bigrams = []\n",
    "    for i in range(len(words_freq_sorted)):\n",
    "        if ' ' in words_freq_sorted[i][0]:\n",
    "            bigrams.append(words_freq_sorted[i][0])\n",
    "            bigrams_count += 1\n",
    "            if (bigrams_count == 50):\n",
    "                break\n",
    "        elif unigrams_count < 50:\n",
    "            unigrams.append(words_freq_sorted[i][0])\n",
    "            unigrams_count += 1\n",
    "    print(\"50 most frequent unigrams are:\\n{}\\n\".format(unigrams))\n",
    "    print(\"50 most frequent bigrams are:\\n{}\\n\".format(bigrams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 most frequent unigrams are:\n",
      "['font', '3d', 'the', 'br', 'td', 'to', 'and', 'nbsp', 'of', 'http', '20', 'size', 'tr', 'in', 'width', 'com', 'you', 'face', 'for', 'border', 'is', 'style', 'this', 'align', 'span', 'href', 'height', 'html', 'color', 'www', 'that', 'on', 'your', 'content', 'table', 'with', 'be', 'div', 'arial', 'it', 'from', 'we', 'or', 'center', 'img', 'as', 'enron', 'src', 'are', 'text']\n",
      "\n",
      "50 most frequent bigrams are:\n",
      "['nbsp nbsp', 'br br', 'font size', 'td tr', 'http www', 'font face', 'td td', 'tr td', '3d http', 'style 3d', 'font td', 'tr tr', 'href 3d', 'font font', 'of the', 'td width', 'color 3d', 'font color', 'img src', 'href http', 'in the', 'arial helvetica', 'width 3d', 'sans serif', 'face 3d', 'size 3d2', 'align center', 'content type', 'helvetica sans', 'bgcolor 3d', 'src http', '1px solid', 'gif width', 'font family', 'tr table', 'size 3d', 'face 3darial', 'br font', 'face 3dverdana', '3d font', 'src 3d', 'span style', 'text html', 'face arial', 'align 3d', 'td align', 'size 3d1', 'if you', 'body html', 'html charset']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_50_freq_bi_uni(all_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it would seems very surprising that 'the' and the other most common words in english don't take all the first places. It's also surprising in second glance, but we need to remember that we are dealing with a lot of emails, which will explain how 'http' got to be in the top ten. Also, we are dealing with a lot of spam, which probably has different word frequencies, which might explain '3d' and 'size', for example. However, how 'font', 'br', 'td', and other non-words (and the weirdest 'nbsp nbsp' which is the most frequent bigram!) got to be so common, is a mystery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent for spam:\n",
      "\n",
      "50 most frequent unigrams are:\n",
      "['font', '3d', 'br', 'td', 'nbsp', 'the', 'size', 'tr', 'to', '20', 'http', 'and', 'width', 'of', 'face', 'border', 'style', 'span', 'align', 'com', 'in', 'color', 'html', 'you', 'href', 'height', 'content', 'is', 'this', 'www', 'div', 'for', 'your', 'arial', 'table', 'center', 'text', 'img', 'class', 'body', 'src', 'strong', 'or', '3d2', '3d0', 'that', 'type', 'with', 'be', 'bgcolor']\n",
      "\n",
      "50 most frequent bigrams are:\n",
      "['nbsp nbsp', 'br br', 'font size', 'td tr', 'font face', 'td td', 'tr td', '3d http', 'style 3d', 'http www', 'font td', 'tr tr', 'href 3d', 'font font', 'color 3d', 'font color', 'td width', 'arial helvetica', 'width 3d', 'face 3d', 'size 3d2', 'sans serif', 'content type', 'align center', 'helvetica sans', '1px solid', 'href http', 'bgcolor 3d', 'font family', 'face 3darial', 'size 3d', 'face 3dverdana', '3d font', 'img src', 'of the', 'br font', 'span style', 'tr table', 'text html', 'align 3d', 'size 3d1', 'td align', 'src 3d', 'body html', 'html charset', 'in the', 'border 3d0', '3d border', 'content 3d', 'face arial']\n",
      "\n",
      "Most frequent for ham:\n",
      "\n",
      "50 most frequent unigrams are:\n",
      "['the', 'to', 'and', 'of', 'in', 'com', 'for', 'enron', 'is', 'on', 'http', 'that', 'you', 'td', 'this', 'it', 'with', 'be', 'ect', '20', 'width', 'have', 'from', 'will', 'www', 'as', 'we', 'at', '3d', 'are', 'by', 'font', 'or', 'tr', 'if', 'not', 'your', 'br', 'height', '09', 'src', 'gif', 'img', 'an', '10', 'hou', 'has', 'was', '2001', 'href']\n",
      "\n",
      "50 most frequent bigrams are:\n",
      "['http www', 'of the', 'in the', 'img src', 'hou ect', 'ect ect', 'src http', 'gif width', 'href http', 'to the', 'on the', 'for the', '09 09', 'td width', 'td tr', 'td td', 'will be', 'cnet com', 'com gif', 'if you', 'to be', 'tr td', 'com click', 'width height', 'enron enron', 'online com', 'http clickthru', 'clickthru online', 'font face', 'enron com', 'with the', 'the company', 'and the', 'pm to', 'at the', '3d http', 'zdnet com', 'www cnet', 'you have', 'tr table', 'that the', 'www zdnet', 'arial helvetica', 'width 3d', 'height td', 'am to', 'message from', 'original message', 'from the', 'font td']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = data.iloc[all_indices]['class'].values.astype(str)\n",
    "print(\"Most frequent for spam:\\n\")\n",
    "spam_indices = [idx for idx in all_indices if y[idx] == 'spam']\n",
    "print_50_freq_bi_uni(spam_indices)\n",
    "\n",
    "print(\"Most frequent for ham:\\n\")\n",
    "ham_indices = [idx for idx in all_indices if y[idx] == 'ham']\n",
    "print_50_freq_bi_uni(ham_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now things look much better: in the 'ham' part, we see results of normal English, although email related. Probably, the spam wasn't what the 'people' call spam, such as advertisements, but it was constructed from pile of meaningless garbage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to get the following results: the best features will be those words that appear only in one of the classes 'spam' and 'ham' - we can be (almost) totally sure when classifying according to these features. Therefore, a possible output might be gibrish for the 'spam' features, since we don't expect to get meaningless words in real emails. In the other direction, a possible output for the best of 'ham' features might be very meaningful words, such as scientific words, which we don't expect to see in spam emails\\SMSs.\n",
    "\n",
    "The coef_ of the classifier holds the weights of the features for the classifiction text (after the training). Therefore, we need to take the indices of the highest and lowest 20 weights, and find the appropriate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = pipeline.get_params()['count_vectorizer'].get_feature_names()\n",
    "top20 = np.argsort(pipeline.get_params()['classifier'].coef_[0])[-20:]\n",
    "low20 = np.argsort(pipeline.get_params()['classifier'].coef_[0])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align\n",
      "span\n",
      "style\n",
      "face\n",
      "br br\n",
      "width\n",
      "of\n",
      "nbsp nbsp\n",
      "and\n",
      "http\n",
      "20\n",
      "to\n",
      "tr\n",
      "size\n",
      "the\n",
      "nbsp\n",
      "td\n",
      "br\n",
      "3d\n",
      "font\n",
      "\n",
      "\n",
      "\n",
      "imac ipod\n",
      "has forward\n",
      "has formally\n",
      "has forgotten\n",
      "has forfeited\n",
      "has forced\n",
      "has followed\n",
      "has folded\n",
      "has fm\n",
      "has fluctuated\n",
      "has flowed\n",
      "has forwarded\n",
      "has flourished\n",
      "has flexmode\n",
      "has fixed\n",
      "has five\n",
      "has firmly\n",
      "has firm\n",
      "has fired\n",
      "has finite\n"
     ]
    }
   ],
   "source": [
    "for idx in top20:\n",
    "    print (feature_names[idx])\n",
    "print(\"\\n\\n\")\n",
    "for idx in low20:\n",
    "    print (feature_names[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, normalization is required for classifiers which are affected by the absolute values of the features, or by the relative differences between the features (there is also a speed improvement for some algorithms by normalization, but as it doesn't change the output, it's less relevant for the question). In the first group we can find regularized algorithms, and in the second group appear k-nearest neighbors, for example. Since irregularized naive bayes and regression algorithms don't fall in both categories, there shouldn't be any improvement by normalizing the extra feature to be in the same range as the others.\n",
    "\n",
    "In general, adding 'good' features (i.e. features that we know by prior information that they are relevant) to the model should improve the learning unless there're too many features which leads to overfitting. In our case, there're so many features as it is (number of word types), that another feature shouldn't do much harm regarding overfitting. Since this we expect this feature to be more predictive then many of the word type features, we expect the learning algorithms to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 6 folds\n",
      "Training for fold 0\n",
      "Testing for fold 0\n",
      "Score for 0: 0.98\n",
      "Confusion matrix for 0: \n",
      "[[3850   22]\n",
      " [ 225 5722]]\n",
      "Training for fold 1\n",
      "Testing for fold 1\n",
      "Score for 1: 0.98\n",
      "Confusion matrix for 1: \n",
      "[[ 7766    41]\n",
      " [  435 11396]]\n",
      "Training for fold 2\n",
      "Testing for fold 2\n",
      "Score for 2: 0.98\n",
      "Confusion matrix for 2: \n",
      "[[11703    58]\n",
      " [  644 17051]]\n",
      "Training for fold 3\n",
      "Testing for fold 3\n",
      "Score for 3: 0.98\n",
      "Confusion matrix for 3: \n",
      "[[15625    81]\n",
      " [  839 22729]]\n",
      "Training for fold 4\n",
      "Testing for fold 4\n",
      "Score for 4: 0.98\n",
      "Confusion matrix for 4: \n",
      "[[19480    95]\n",
      " [ 1035 28482]]\n",
      "Training for fold 5\n",
      "Testing for fold 5\n",
      "Score for 5: 0.98\n",
      "Confusion matrix for 5: \n",
      "[[23423   116]\n",
      " [ 1227 34144]]\n",
      "Total emails classified: 58910\n",
      "Score: 0.9807132210394963\n",
      "Confusion matrix:\n",
      "[[23423   116]\n",
      " [ 1227 34144]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class LengthTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [[len(datum)] for datum in X]\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "def build_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "            ('doc_length', LengthTransformer())\n",
    "        ])),\n",
    "        ('classifier',         MultinomialNB())\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def train(data = None, n_folds = 6):\n",
    "    if data is None:\n",
    "        print(\"Loading data...\")\n",
    "        data = load_data()\n",
    "        print(\"Data loaded\")\n",
    "    k_fold = KFold(n_splits = n_folds)\n",
    "    pipeline = build_pipeline()\n",
    "    scores = []\n",
    "    confusion = numpy.array([[0, 0], [0, 0]])\n",
    "    print(\"Training with %d folds\" % n_folds)\n",
    "    for i, (train_indices, test_indices) in enumerate(k_fold.split(data)):\n",
    "        train_text = data.iloc[train_indices]['text'].values\n",
    "        train_y = data.iloc[train_indices]['class'].values.astype(str)\n",
    "\n",
    "        test_text = data.iloc[test_indices]['text'].values\n",
    "        test_y = data.iloc[test_indices]['class'].values.astype(str)\n",
    "\n",
    "        print(\"Training for fold %d\" % i)\n",
    "        pipeline.fit(train_text, train_y)\n",
    "        print(\"Testing for fold %d\" % i)\n",
    "        predictions = pipeline.predict(test_text)\n",
    "\n",
    "        confusion += confusion_matrix(test_y, predictions)\n",
    "        score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "        scores.append(score)\n",
    "        print(\"Score for %d: %2.2f\" % (i, score))\n",
    "        print(\"Confusion matrix for %d: \" % i)\n",
    "        print(confusion)\n",
    "\n",
    "    print('Total emails classified:', len(data))\n",
    "    print('Score:', sum(scores)/len(scores))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion)\n",
    "    return pipeline\n",
    "\n",
    "pipeline = train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, that the fscore before the change was 0.9782823670397819, which means approximately 10% error reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will try to tag words in a text as: PER, LOC, ORG, MISC and O for person name, location, organization and none of this, respectively. Each word can tagged as B-X for word that begins the X name, I-X for word that into X name and O for words that are not names at all. \n",
    "for this task we use conll2002 corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will transform every word to dictionary of it's features. In the first try we wikk try this:\n",
    "    - The word form (the string as it appears in the sentence)\n",
    "    - The POS of the word (which is provided in the dataset)\n",
    "    - ORT - a feature that captures the orthographic (letter) structure of the word. It can have any of the following values: number, contains-digit, contains-hyphen, capitalized, all-capitals, URL, punctuation, regular.\n",
    "    - prefix1: first letter of the word\n",
    "    - prefix2: first two letters of the word\n",
    "    - prefix3: first three letters of the word\n",
    "    - suffix1: last letter of the word\n",
    "    - suffix2: last two letters of the word\n",
    "    - suffix3: last three letters of the word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     /users/studs/bsc/2016/berendb/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2002.zip.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def features2vector(data_sent):\n",
    "    features = [sent2features(sent) for sent in data_sent]\n",
    "    f_feathures = [item for sent in features for item in sent]\n",
    "    return f_feathures\n",
    "\n",
    "def labels2vector(data_sent):\n",
    "    labels = [sent2labels(sent) for sent in data_sent]\n",
    "    f_labels = [item for sent in labels for item in sent]\n",
    "    return f_labels\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'word form': word,\n",
    "        'pos': postag,\n",
    "        'ORT number': 1 if word.isdigit() else 0,\n",
    "        'ORT contain number': 1 if hasNumbers(word) else 0,\n",
    "        'ORT all-capitals': 1 if word.isupper() else 0,\n",
    "        'ORT capitalized': 1 if word[:1].isupper() else 0,\n",
    "        'ORT punctuation': 1 if (c in string.punctuation() for c in word) else 0,\n",
    "        'preffix1': word[:1],\n",
    "        'preffix2': word[:2],\n",
    "        'preffix3': word[:3],\n",
    "        'suffix1': word[-1:],\n",
    "        'suffix2': word[-2:],\n",
    "        'suffix3': word[-3:],\n",
    "    }           \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word form': 'Melbourne',\n",
       " 'pos': 'NP',\n",
       " 'ORT number': 0,\n",
       " 'ORT contain number': 0,\n",
       " 'ORT all-capitals': 0,\n",
       " 'ORT capitalized': 1,\n",
       " 'ORT punctuation': 1,\n",
       " 'preffix1': 'M',\n",
       " 'preffix2': 'Me',\n",
       " 'preffix3': 'Mel',\n",
       " 'suffix1': 'e',\n",
       " 'suffix2': 'ne',\n",
       " 'suffix3': 'rne'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "dev_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "sent2features(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an example for a transformed word. Every dictionary that present word, will be transformed into a vector that captures this features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "\n",
    "def train_model(Xtrain, Ytrain):\n",
    "    clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=True)),\n",
    "\t('classifier', LogisticRegression())\n",
    "    ])\n",
    "    clf.fit(Xtrain, Ytrain)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/studs/bsc/2016/berendb/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/users/studs/bsc/2016/berendb/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9441134806822813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.65      0.70      0.67      1084\n",
      "      B-MISC       0.49      0.42      0.45       339\n",
      "       B-ORG       0.76      0.82      0.79      1400\n",
      "       B-PER       0.64      0.68      0.66       735\n",
      "       I-LOC       0.51      0.19      0.27       325\n",
      "      I-MISC       0.43      0.14      0.21       557\n",
      "       I-ORG       0.56      0.27      0.37      1104\n",
      "       I-PER       0.56      0.64      0.60       634\n",
      "           O       0.98      1.00      0.99     45355\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     51533\n",
      "   macro avg       0.62      0.54      0.56     51533\n",
      "weighted avg       0.94      0.94      0.94     51533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xtrain = features2vector(train_sents)\n",
    "Ytrain = labels2vector(train_sents)\n",
    "clf = train_model(Xtrain, Ytrain)\n",
    "Xtest = features2vector(dev_sents)\n",
    "Ypred = clf.predict(Xtest)\n",
    "Ytrue = labels2vector(dev_sents)\n",
    "print(clf.score(Xtest, Ytrue))\n",
    "print(metrics.classification_report(Ytrue, Ypred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Label</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>B-MISC</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>I-MISC</th>\n",
       "      <th>I-ORG</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>O</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>763</td>\n",
       "      <td>4</td>\n",
       "      <td>125</td>\n",
       "      <td>43</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>58</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MISC</th>\n",
       "      <td>16</td>\n",
       "      <td>141</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>93</td>\n",
       "      <td>23</td>\n",
       "      <td>1147</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>503</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>117</td>\n",
       "      <td>12</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>99</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MISC</th>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>71</td>\n",
       "      <td>25</td>\n",
       "      <td>275</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>128</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>49</td>\n",
       "      <td>301</td>\n",
       "      <td>52</td>\n",
       "      <td>399</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>102</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>405</td>\n",
       "      <td>26</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>45255</td>\n",
       "      <td>45355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1182</td>\n",
       "      <td>285</td>\n",
       "      <td>1504</td>\n",
       "      <td>781</td>\n",
       "      <td>120</td>\n",
       "      <td>179</td>\n",
       "      <td>542</td>\n",
       "      <td>720</td>\n",
       "      <td>46220</td>\n",
       "      <td>51533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Label  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER  \\\n",
       "True Label                                                                  \n",
       "B-LOC              763       4    125     43     15      13     20     43   \n",
       "B-MISC              16     141     55      6      0      17     27     13   \n",
       "B-ORG               93      23   1147     43      2       1     30     29   \n",
       "B-PER               52       4     21    503     10       4     12    117   \n",
       "I-LOC               54       7     22     24     61       1     28     29   \n",
       "I-MISC              33      19     29     24      4      77     71     25   \n",
       "I-ORG              128      66     68     27     14      49    301     52   \n",
       "I-PER               34       3      9    102     14      11     30    405   \n",
       "O                    9      18     28      9      0       6     23      7   \n",
       "All               1182     285   1504    781    120     179    542    720   \n",
       "\n",
       "Predicted Label      O    All  \n",
       "True Label                     \n",
       "B-LOC               58   1084  \n",
       "B-MISC              64    339  \n",
       "B-ORG               32   1400  \n",
       "B-PER               12    735  \n",
       "I-LOC               99    325  \n",
       "I-MISC             275    557  \n",
       "I-ORG              399   1104  \n",
       "I-PER               26    634  \n",
       "O                45255  45355  \n",
       "All              46220  51533  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"confusion matrix:\")\n",
    "y_true = pd.Series(Ytrue)\n",
    "y_pred = pd.Series(Ypred)\n",
    "pd.crosstab(y_true, y_pred, rownames=['True Label'], colnames=['Predicted Label'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seems pretty good by first look, but in second one you can see that the f-score of every tag except 'O', is not so high. The problem here is that 'O' tag capture most of the words, so even if it's predict everithing as 'O' the total  f-score will be pretty high.\n",
    "Let's add some more features and see if there is improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2extend_features(sent, i):\n",
    "    feature = word2features(sent, i)\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    prev_word = ''\n",
    "    prev_postag = ''\n",
    "    next_word = ''\n",
    "    next_postag = ''\n",
    "    if i > 0:\n",
    "        prev_word = sent[i-1][0]\n",
    "        prev_postag = sent[i-1][1]\n",
    "    if i < len(sent) - 1:\n",
    "        next_word = sent[i+1][0]\n",
    "        next_postag = sent[i+1][1]\n",
    "    ext = {\n",
    "        'has hyphen': 1 if '-' in word else 0,\n",
    "        'place': i,\n",
    "        'sent len': len(sent),\n",
    "        'prev word': prev_word,\n",
    "        'prev postag': prev_postag,\n",
    "        'next word': next_word,\n",
    "        'next postag': next_postag\n",
    "    }\n",
    "    feature.update(ext)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extends_features2vector(data_sent):\n",
    "    features = [sent2extendsfeatures(sent) for sent in data_sent]\n",
    "    f_feathures = [item for sent in features for item in sent]\n",
    "    return f_feathures\n",
    "\n",
    "def sent2extendsfeatures(sent):\n",
    "    return [word2extend_features(sent, i) for i in range(len(sent))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9588613121689015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.73      0.71      0.72      1084\n",
      "      B-MISC       0.54      0.42      0.47       339\n",
      "       B-ORG       0.78      0.83      0.81      1400\n",
      "       B-PER       0.83      0.87      0.85       735\n",
      "       I-LOC       0.72      0.39      0.50       325\n",
      "      I-MISC       0.59      0.27      0.38       557\n",
      "       I-ORG       0.74      0.52      0.61      1104\n",
      "       I-PER       0.77      0.90      0.83       634\n",
      "           O       0.98      1.00      0.99     45355\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     51533\n",
      "   macro avg       0.74      0.66      0.68     51533\n",
      "weighted avg       0.95      0.96      0.96     51533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xtrain = extends_features2vector(train_sents)\n",
    "Ytrain = labels2vector(train_sents)\n",
    "clf = train_model(Xtrain, Ytrain)\n",
    "Xtest = extends_features2vector(dev_sents)\n",
    "Ypred = clf.predict(Xtest)\n",
    "Ytrue = labels2vector(dev_sents)\n",
    "print(clf.score(Xtest, Ytrue))\n",
    "print(metrics.classification_report(Ytrue, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Label</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>B-MISC</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>I-MISC</th>\n",
       "      <th>I-ORG</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>O</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>768</td>\n",
       "      <td>6</td>\n",
       "      <td>132</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MISC</th>\n",
       "      <td>23</td>\n",
       "      <td>141</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>64</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>97</td>\n",
       "      <td>15</td>\n",
       "      <td>1168</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>643</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>70</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MISC</th>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>153</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>240</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>81</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>41</td>\n",
       "      <td>579</td>\n",
       "      <td>53</td>\n",
       "      <td>230</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>45265</td>\n",
       "      <td>45355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1059</td>\n",
       "      <td>263</td>\n",
       "      <td>1492</td>\n",
       "      <td>774</td>\n",
       "      <td>176</td>\n",
       "      <td>258</td>\n",
       "      <td>787</td>\n",
       "      <td>739</td>\n",
       "      <td>45985</td>\n",
       "      <td>51533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Label  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER  \\\n",
       "True Label                                                                  \n",
       "B-LOC              768       6    132     44     14       7     33     19   \n",
       "B-MISC              23     141     60      5      1      14     19     12   \n",
       "B-ORG               97      15   1168     48      1       8     22     19   \n",
       "B-PER               24       3     19    643      2       2      7     23   \n",
       "I-LOC               29       4     11      7    126      10     40     28   \n",
       "I-MISC              20      28     22      6     12     153     65     11   \n",
       "I-ORG               81      50     51      4     15      41    579     53   \n",
       "I-PER                9       2      1      9      2       6     14    570   \n",
       "O                    8      14     28      8      3      17      8      4   \n",
       "All               1059     263   1492    774    176     258    787    739   \n",
       "\n",
       "Predicted Label      O    All  \n",
       "True Label                     \n",
       "B-LOC               61   1084  \n",
       "B-MISC              64    339  \n",
       "B-ORG               22   1400  \n",
       "B-PER               12    735  \n",
       "I-LOC               70    325  \n",
       "I-MISC             240    557  \n",
       "I-ORG              230   1104  \n",
       "I-PER               21    634  \n",
       "O                45265  45355  \n",
       "All              45985  51533  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"confusion matrix:\")\n",
    "y_true = pd.Series(Ytrue)\n",
    "y_pred = pd.Series(Ypred)\n",
    "pd.crosstab(y_true, y_pred, rownames=['True Label'], colnames=['Predicted Label'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results got better, some by far. Total fscore improved from 0.944 to 0.958 which is an error reduction of 25% approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3: Illegal Tags Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictor predicts without optimizing sequence of tags, so there is a lot of illegal sequence in the prediction tags. We provide code that gets prediction of tags sequence and returns a table where the entry i,j represent the frequency of the mistake of kind j in type i. The mistake kinds of type X are: O-IX, IX-IY, BX-IY, IY-IX, BY-IX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def find_illegal_seq(Ypred, types):\n",
    "    illegl = np.zeros((4,6))\n",
    "    types_counter = np.zeros((len(types),), dtype=int)\n",
    "    for t in types:\n",
    "        for i in range(1, len(Ypred)):\n",
    "            p = Ypred[i]\n",
    "            if p[2:] == t:\n",
    "                types_counter[types.index(t)]+=1\n",
    "                if p[0] == 'I':\n",
    "                    if Ypred[i-1] == 'O':\n",
    "                        illegl[types.index(t)][0] += 1\n",
    "                    elif not Ypred[i-1][2:] == t:\n",
    "                        l = (3 if Ypred[i-1][0] == 'I'  else 4)\n",
    "                        illegl[types.index(t)][l] += 1\n",
    "                    if i < len(Ypred) and not (Ypred[i+1])[2:] == t:\n",
    "                        illegl[types.index(t)][1] += 1\n",
    "                elif p[0] == 'B':\n",
    "                    if i < len(Ypred)-1 and not ((Ypred[i+1])[2:] == t):\n",
    "                        illegl[types.index(t)][2] += 1\n",
    "    \n",
    "    for i in range(len(types)):\n",
    "        for j in  range(5):\n",
    "            illegl[i][j] = round(illegl[i][j]/types_counter[i], 2)\n",
    "            \n",
    "    return illegl\n",
    "\n",
    "def print_illegal_matrix(illegal, types):\n",
    "    print('Mistake type\\tO-IX\\tIX-IY\\tBX-IY\\tIY-IX\\tBY-IX')\n",
    "    print('Tag type')\n",
    "    for i in range(len(types)):\n",
    "        print(types[i]+'\\t\\t'+str(illegal[i][0])+'\\t'+str(illegal[i][1])+'\\t'+str(illegal[i][2])+'\\t'+str(illegal[i][3])+'\\t'+str(illegal[i][4]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistake type\tO-IX\tIX-IY\tBX-IY\tIY-IX\tBY-IX\n",
      "Tag type\n",
      "LOC\t\t0.03\t0.12\t0.78\t0.01\t0.01\n",
      "MISC\t\t0.17\t0.38\t0.36\t0.04\t0.05\n",
      "ORG\t\t0.07\t0.25\t0.51\t0.01\t0.02\n",
      "PER\t\t0.07\t0.42\t0.19\t0.01\t0.02\n"
     ]
    }
   ],
   "source": [
    "types = ['LOC', 'MISC', 'ORG', 'PER']\n",
    "i = find_illegal_seq(Ypred, types)\n",
    "print_illegal_matrix(i, types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the major problems were when continuing, i.e. transitioning from BX or IX to IY. The explanation might be that the model is generally good with identifying the sequences of NER, including cases when the entity was composed from more than one word, but the similarities between the entities (Nouns, start with capital letters) caused mislabeling the sub-parts of the entity correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.2: Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to improve our NER task by adding word vector as extra features of the word. we treat every entry in the vector as new feature in the features of the word. In case that we can, we add the word vectors of the previous and next words, so we expect that the tags sequence will improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "vec_file_name = '../wiki.es.vec'\n",
    "wordvectors = KeyedVectors.load_word2vec_format(vec_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2extend_features_with_wv(sent, i):\n",
    "    feature = word2extend_features(sent, i)\n",
    "    word = sent[i][0]\n",
    "    try:\n",
    "        wv =  wordvectors.wv[word]\n",
    "        feature.update(wv2feature(wv))\n",
    "    except:\n",
    "        pass            \n",
    "    if i > 0:\n",
    "        prev_word = sent[i-1][0]\n",
    "        try:\n",
    "            pwv =  wordvectors.wv[prev_word]\n",
    "            feature.update(wv2feature(pwv))\n",
    "        except:\n",
    "            pass\n",
    "    if i < len(sent) - 1:\n",
    "        next_word = sent[i+1][0]\n",
    "        try:\n",
    "            nwv =  wordvectors.wv[next_word]\n",
    "            feature.update(wv2feature(nwv))\n",
    "        except:\n",
    "            pass\n",
    "    return feature\n",
    "\n",
    "def wv2feature(wv):\n",
    "    f = {}\n",
    "    for i in range(len(wv)):\n",
    "        f_i = {\n",
    "            'f'+str(i): wv[i]\n",
    "        }\n",
    "        f.update(f_i)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extends_features2vector2(data_sent):\n",
    "    features = [sent2extendsfeatures2(sent) for sent in data_sent]\n",
    "    f_feathures = [item for sent in features for item in sent]\n",
    "    return f_feathures\n",
    "\n",
    "def sent2extendsfeatures2(sent):\n",
    "    return [word2extend_features_with_wv(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9588613121689015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.73      0.71      0.72      1084\n",
      "      B-MISC       0.54      0.42      0.47       339\n",
      "       B-ORG       0.78      0.83      0.81      1400\n",
      "       B-PER       0.83      0.87      0.85       735\n",
      "       I-LOC       0.72      0.39      0.50       325\n",
      "      I-MISC       0.59      0.27      0.38       557\n",
      "       I-ORG       0.74      0.52      0.61      1104\n",
      "       I-PER       0.77      0.90      0.83       634\n",
      "           O       0.98      1.00      0.99     45355\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     51533\n",
      "   macro avg       0.74      0.66      0.68     51533\n",
      "weighted avg       0.95      0.96      0.96     51533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xtrain = extends_features2vector2(train_sents)\n",
    "Ytrain = labels2vector(train_sents)\n",
    "clf = train_model(Xtrain, Ytrain)\n",
    "Xtest = extends_features2vector2(dev_sents)\n",
    "Ypred = clf.predict(Xtest)\n",
    "Ytrue = labels2vector(dev_sents)\n",
    "print(clf.score(Xtest, Ytrue))\n",
    "print(metrics.classification_report(Ytrue, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Label</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>B-MISC</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>I-MISC</th>\n",
       "      <th>I-ORG</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>O</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>768</td>\n",
       "      <td>6</td>\n",
       "      <td>132</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MISC</th>\n",
       "      <td>23</td>\n",
       "      <td>141</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>64</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>97</td>\n",
       "      <td>15</td>\n",
       "      <td>1168</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>643</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>70</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MISC</th>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>153</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>240</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>81</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>41</td>\n",
       "      <td>579</td>\n",
       "      <td>53</td>\n",
       "      <td>230</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>45265</td>\n",
       "      <td>45355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1059</td>\n",
       "      <td>263</td>\n",
       "      <td>1492</td>\n",
       "      <td>774</td>\n",
       "      <td>176</td>\n",
       "      <td>258</td>\n",
       "      <td>787</td>\n",
       "      <td>739</td>\n",
       "      <td>45985</td>\n",
       "      <td>51533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Label  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER  \\\n",
       "True Label                                                                  \n",
       "B-LOC              768       6    132     44     14       7     33     19   \n",
       "B-MISC              23     141     60      5      1      14     19     12   \n",
       "B-ORG               97      15   1168     48      1       8     22     19   \n",
       "B-PER               24       3     19    643      2       2      7     23   \n",
       "I-LOC               29       4     11      7    126      10     40     28   \n",
       "I-MISC              20      28     22      6     12     153     65     11   \n",
       "I-ORG               81      50     51      4     15      41    579     53   \n",
       "I-PER                9       2      1      9      2       6     14    570   \n",
       "O                    8      14     28      8      3      17      8      4   \n",
       "All               1059     263   1492    774    176     258    787    739   \n",
       "\n",
       "Predicted Label      O    All  \n",
       "True Label                     \n",
       "B-LOC               61   1084  \n",
       "B-MISC              64    339  \n",
       "B-ORG               22   1400  \n",
       "B-PER               12    735  \n",
       "I-LOC               70    325  \n",
       "I-MISC             240    557  \n",
       "I-ORG              230   1104  \n",
       "I-PER               21    634  \n",
       "O                45265  45355  \n",
       "All              45985  51533  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"confusion matrix:\")\n",
    "y_true = pd.Series(Ytrue)\n",
    "y_pred = pd.Series(Ypred)\n",
    "pd.crosstab(y_true, y_pred, rownames=['True Label'], colnames=['Predicted Label'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistake type\tO-IX\tIX-IY\tBX-IY\tIY-IX\tBY-IX\ttotal\n",
      "Tag type\n",
      "LOC\t\t0.03\t0.12\t0.78\t0.01\t0.01\t0.95\n",
      "MISC\t\t0.17\t0.38\t0.36\t0.04\t0.05\t1.0\n",
      "ORG\t\t0.07\t0.25\t0.51\t0.01\t0.02\t0.86\n",
      "PER\t\t0.07\t0.42\t0.19\t0.01\t0.02\t0.71\n"
     ]
    }
   ],
   "source": [
    "i = find_illegal_seq(Ypred, types)\n",
    "print_illegal_matrix(i, types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a little improvement in the frequency of the illegal sequences. It still high. Maybe if we add the word vector of every word in the sentence, and there was a vector for each word, the resuly will be more good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the work, we inserted it to the program we mentioned at the end of our previous work. The computer said it reminds him of the following joke:\n",
    "In the 19702, two KGB officers walk in a street somewhere in Moscow. Suddenly, they see a guy runs past them. One of them pulls out his gun, and shoots him. \n",
    "The other officer says: \"Hey, why did you do that?\"\n",
    "The first replies: \"What do you mean? It's curfew.\"\n",
    "The second: \"Yeah, but it hadn't started yet.\"\n",
    "The first answers: \"Oh, yeah, but this guy is a friend of mine. He won't make it home in time\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
